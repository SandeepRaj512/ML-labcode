{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question A1:\n",
    "# Develop a perceptron of a particular data\n",
    "\n",
    "# Data for logic gates\n",
    "data_gates_input = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an activation function\n",
    "def activation_step_function(number):\n",
    "    if number >= 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final weights of Perceptron are: \n",
      "\n",
      "W0:  -0.10000000000000765\n",
      "W1:  0.1000000000000001\n",
      "W2:  0.05000000000000032\n",
      "Total number of epochs in the system:  130\n"
     ]
    }
   ],
   "source": [
    "# Applying data on the and gate\n",
    "weight0 = 10\n",
    "weight1 = 0.2\n",
    "weight2 = -0.75\n",
    "learning_rate = 0.05\n",
    "\n",
    "# Applying the perceptron equation\n",
    "target_output = [0, 0, 0, 1]\n",
    "error_count = []\n",
    "epochs_count = 0\n",
    "converged = False\n",
    "while not converged:\n",
    "    total_error = 0\n",
    "    for i in range(len(data_gates_input)):\n",
    "        data_a = data_gates_input[i, 0]\n",
    "        data_b = data_gates_input[i, 1]\n",
    "        perceptor_equation = weight0 + data_a * weight1 + data_b * weight2\n",
    "        actual_output = activation_step_function(perceptor_equation)\n",
    "        error_value = target_output[i] - actual_output\n",
    "        total_error += error_value ** 2\n",
    "\n",
    "        weight0 += learning_rate * error_value\n",
    "        weight1 += learning_rate * error_value * data_a\n",
    "        weight2 += learning_rate * error_value * data_b\n",
    "    error_count.append(total_error)\n",
    "    if total_error == 0:\n",
    "        converged = True\n",
    "    epochs_count += 1\n",
    "print(\"Final weights of Perceptron are: \\n\")\n",
    "print(\"W0: \", weight0)\n",
    "print(\"W1: \", weight1)\n",
    "print(\"W2: \", weight2)\n",
    "print(\"Total number of epochs in the system: \", epochs_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the error values vs Epoch values graph\n",
    "plt.plot(range(epochs_count), error_count, color='green')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Epoch count vs Error value for AND logic')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question A2:\n",
    "# Repeat the Question A1 with different activation functions\n",
    "\n",
    "# Defining the bipolar activation function\n",
    "def activation_bipolar_function(number):\n",
    "    if number > 0 :\n",
    "        return 1\n",
    "    elif number == 0 :\n",
    "        return 0\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Sigmoid activation function\n",
    "def activation_sigmoid_function(number):\n",
    "    return 1 / (1 + math.e ** (-number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the sigmoid derivative function\n",
    "def activation_sigmoid_derivative_function(number):\n",
    "    return number * (1 - number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the ReLU activation function\n",
    "def activation_relu_function(number):\n",
    "    if number > 0 :\n",
    "        return number\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the function to apply a percetron\n",
    "def perceptron(data_gates_input, target_output, w0, w1, w2, alpha, function):\n",
    "    error_count = []\n",
    "    epochs_count = 0\n",
    "    converged = False\n",
    "    while not converged and epochs_count != 1000:\n",
    "        total_error = 0\n",
    "        for i in range(len(data_gates_input)):\n",
    "            data_a = data_gates_input[i, 0]\n",
    "            data_b = data_gates_input[i, 1]\n",
    "            perceptor_equation = w0 + data_a * w1 + data_b * w2\n",
    "            if function == 'step':\n",
    "                actual_output = activation_step_function(perceptor_equation)\n",
    "            if function == 'bipolar':\n",
    "                actual_output = activation_bipolar_function(perceptor_equation)\n",
    "            if function == 'sigmoid':\n",
    "                actual_output = activation_sigmoid_function(perceptor_equation)\n",
    "            if function == 'relu':\n",
    "                actual_output = activation_relu_function(perceptor_equation)\n",
    "            error_value = target_output[i] - actual_output\n",
    "            total_error += error_value ** 2\n",
    "\n",
    "            w0 += alpha * error_value\n",
    "            w1 += alpha * error_value * data_a\n",
    "            w2 += alpha * error_value * data_b\n",
    "        error_count.append(total_error)\n",
    "        if total_error <= 0.002:\n",
    "            converged = True\n",
    "        epochs_count += 1\n",
    "    print('Final weights of Perceptron are: \\n')\n",
    "    print('W0: ', w0, '\\n')\n",
    "    print('W1: ', w1, '\\n')\n",
    "    print('W2: ', w2, '\\n')\n",
    "    return epochs_count, error_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final weights of Perceptron are: \n",
      "\n",
      "W0:  -0.0999999999999975 \n",
      "\n",
      "W1:  0.09999999999999902 \n",
      "\n",
      "W2:  0.05000000000000643 \n",
      "\n",
      "The number of epochs are:  1000\n",
      "The error counts of the bipolar step function perceptron are:  [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 7, 7, 3, 7, 7, 7, 7, 7, 3, 7, 7, 7, 7, 7, 3, 7, 7, 7, 7, 3, 7, 7, 7, 7, 7, 3, 7, 7, 7, 7, 7, 3, 7, 7, 7, 7, 7, 3, 7, 7, 7, 7, 7, 3, 7, 7, 7, 7, 7, 3, 7, 7, 7, 7, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 3, 3, 7, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3, 7, 3]\n",
      "Final weights of Perceptron are: \n",
      "\n",
      "W0:  -6.1353130787344305 \n",
      "\n",
      "W1:  3.970833206406072 \n",
      "\n",
      "W2:  3.9639483472599903 \n",
      "\n",
      "The number of epochs are:  1000\n",
      "The error counts of the sigmoid step function perceptron are:  [2.999625015361064, 2.9995474263162407, 2.999453546947369, 2.9993399189517116, 2.999202344929722, 2.9990357304712503, 2.9988338930083946, 2.998589330816093, 2.998292944460943, 2.997933701894788, 2.997498237380636, 2.9969703737382853, 2.996330557373923, 2.995555196823515, 2.994615899081537, 2.993478605356695, 2.99210264146686, 2.990439721359621, 2.988432980167215, 2.9860161723002956, 2.983113258040588, 2.9796387259987385, 2.975499160690847, 2.9705967521091057, 2.964835614804775, 2.958131840945436, 2.9504279799538944, 2.9417118602344074, 2.932038073432869, 2.921547936340895, 2.91048077062355, 2.8991672373012833, 2.887996427160953, 2.8773544581480017, 2.8675432602275444, 2.8586999259932186, 2.850742523701791, 2.84336240893351, 2.836067228720484, 2.8282610448252394, 2.819337391385439, 2.808761477684028, 2.796126358083128, 2.7811787778787624, 2.7638187698926497, 2.7440811457067897, 2.722107437578669, 2.6981151815623376, 2.6723691126098696, 2.6451567292833658, 2.6167691232761827, 2.587486982418587, 2.5575711503130987, 2.5272569160653413, 2.4967511886653497, 2.4662317917885033, 2.435848237610643, 2.4057234685489735, 2.3759561759956656, 2.3466234076264954, 2.317783258037931, 2.289477502533706, 2.261734083339107, 2.234569394174868, 2.2079903356228634, 2.1819961323594046, 2.1565799160122205, 2.131730085626784, 2.1074314626902106, 2.0836662602686933, 2.0604148867627585, 2.037656604597713, 2.0153700632356903, 1.9935337245086422, 1.9721261966372057, 1.9511264915665731, 1.930514218519403, 1.9102697250047571, 1.8903741949730168, 1.8708097123927852, 1.8515592972569372, 1.8326069199019104, 1.8139374985417747, 1.795536884067425, 1.7773918354302147, 1.7594899883063486, 1.741819819211182, 1.7243706067893538, 1.7071323916364705, 1.6900959357006942, 1.6732526820590632, 1.6565947156556415, 1.6401147254196202, 1.6238059680452188, 1.6076622336063329, 1.5916778130929146, 1.5758474678891727, 1.5601664011626104, 1.5446302310948976, 1.5292349658583388, 1.5139769802232088, 1.4988529936699193, 1.483860049874424, 1.4689954974343096, 1.4542569717057077, 1.439642377626636, 1.4251498734099826, 1.410777854998482, 1.3965249411842247, 1.3823899593060602, 1.3683719314493847, 1.3544700610839384, 1.3406837200861426, 1.3270124361029998, 1.3134558802244904, 1.300013854940608, 1.2866862823676004, 1.273473192735532, 1.2603747131359389, 1.2473910565340667, 1.2345225110549705, 1.221769429556634, 1.2091322195062222, 1.1966113331777315, 1.1842072581905994, 1.1719205084094286, 1.1597516152248772, 1.1477011192350575, 1.1357695623455601, 1.1239574803045242, 1.1122653956871051, 1.1006938113413325, 1.0892432043047369, 1.0779140201983872, 1.0667066681021202, 1.0556215159118594, 1.0446588861770825, 1.033819052413689, 1.0231022358848678, 1.0125086028400345, 1.0020382621995796, 0.9916912636710357, 0.981467596280357, 0.9713671873003437, 0.9613899015568005, 0.9515355410918488, 0.9418038451628487, 0.9321944905547062, 0.9227070921828389, 0.913341203963837, 0.9040963199307774, 0.8949718755702992, 0.8859672493588331, 0.8770817644758504, 0.8683146906725756, 0.859665246275326, 0.851132600303452, 0.8427158746827462, 0.8344141465361572, 0.8262264505346559, 0.8181517812921578, 0.8101890957894841, 0.8023373158134328, 0.7945953303981219, 0.786961998256851, 0.7794361501937888, 0.7720165914858388, 0.7647021042260349, 0.7574914496208001, 0.7503833702343203, 0.7433765921741815, 0.7364698272132424, 0.7296617748435215, 0.7229511242585964, 0.716336556261715, 0.7098167450974524, 0.7033903602053297, 0.6970560678943554, 0.6908125329379362, 0.6846584200890513, 0.6785923955159824, 0.6726131281592482, 0.6667192910107143, 0.6609095623161201, 0.6551826267025187, 0.6495371762323239, 0.6439719113858473, 0.6384855419743551, 0.6330767879857917, 0.6277443803654277, 0.6224870617337547, 0.617303587044016, 0.6121927241817914, 0.6071532545090796, 0.6021839733553318, 0.5972836904578758, 0.5924512303541575, 0.587685432728198, 0.5829851527136226, 0.5783492611555798, 0.5737766448338109, 0.5692662066490795, 0.5648168657751005, 0.5604275577780566, 0.556097234705706, 0.5518248651480283, 0.5476094342712761, 0.5434499438272309, 0.5393454121393879, 0.535294874067716, 0.5312973809535791, 0.5273520005463169, 0.5234578169129224, 0.5196139303321822, 0.5158194571745736, 0.5120735297691463, 0.5083752962585568, 0.5047239204433505, 0.5011185816165339, 0.49755847438941814, 0.49404280850965127, 0.4905708086723137, 0.48714171432488595, 0.4837547794668575, 0.4804092724446889, 0.4771044757427982, 0.4738396857711976, 0.4706142126503611, 0.4674273799938668, 0.4642785246893193, 0.4611669966780183, 0.45809215873380943, 0.4550533862415181, 0.45205006697533406, 0.449081600877493, 0.446147399837565, 0.44324688747263985, 0.4403794989086717, 0.43754468056322415, 0.43474188992983587, 0.43197059536420246, 0.42923027587235774, 0.4265204209010147, 0.4238405301302111, 0.42119011326838995, 0.4185686898500297, 0.41597578903592597, 0.41341094941621415, 0.41087371881620743, 0.40836365410512004, 0.4058803210077291, 0.40342329391902165, 0.40099215572186586, 0.39858649760773573, 0.39620591890051027, 0.3938500268833631, 0.39151843662875363, 0.38921077083151856, 0.3869266596450649, 0.3846657405206563, 0.3824276580497823, 0.38021206380959094, 0.3780186162113713, 0.37584698035205677, 0.37369682786872993, 0.3715678367960942, 0.3694596914268857, 0.3673720821751905, 0.3653047054426316, 0.36325726348739085, 0.36122946429602387, 0.35922102145803364, 0.35723165404315516, 0.35526108648131594, 0.3533090484452261, 0.3513752747355544, 0.34945950516864926, 0.3475614844667577, 0.34568096215069893, 0.34381769243494875, 0.3419714341250877, 0.3401419505175698, 0.338329009301767, 0.3365323824642438, 0.3347518461952183, 0.33298718079716527, 0.3312381705955175, 0.329504603851421, 0.3277862726765025, 0.326082972949605, 0.3243945042354497, 0.3227206697051841, 0.3210612760587711, 0.31941613344918396, 0.3177850554083615, 0.31616785877488907, 0.31456436362336204, 0.31297439319539766, 0.3113977738322543, 0.30983433490902335, 0.3082839087703566, 0.30674633066769386, 0.30522143869795526, 0.3037090737436646, 0.3022090794144705, 0.3007213019900305, 0.2992455903642284, 0.29778179599069066, 0.2963297728295738, 0.29488937729558906, 0.2934604682072375, 0.2920429067372249, 0.29063655636402835, 0.2892412828245874, 0.2878569540680921, 0.2864834402108405, 0.2851206134921416, 0.28376834823123653, 0.28242652078521463, 0.28109500950789845, 0.27977369470967606, 0.2784624586182567, 0.27716118534032624, 0.275869760824082, 0.27458807282262443, 0.27331601085818596, 0.27205346618717363, 0.27080033176601015, 0.269556502217749, 0.26832187379944783, 0.2670963443702814, 0.2658798133603733, 0.2646721817403323, 0.26347335199147404, 0.2622832280767119, 0.26110171541210025, 0.25992872083901414, 0.25876415259695207, 0.2576079202969421, 0.25645993489554103, 0.2553201086694099, 0.25418835519045213, 0.2530645893015016, 0.2519487270925445, 0.250840685877466, 0.24974038417130623, 0.24864774166801318, 0.24756267921868264, 0.24648511881027027, 0.24541498354476823, 0.24435219761883104, 0.24329668630384313, 0.24224837592641565, 0.24120719384930317, 0.24017306845272937, 0.23914592911611252, 0.2381257062001809, 0.23711233102946927, 0.23610573587518707, 0.23510585393844835, 0.23411261933385735, 0.23312596707343924, 0.2321458330509078, 0.23117215402626412, 0.23020486761071554, 0.22924391225191015, 0.22828922721947756, 0.227340752590869, 0.22639842923749093, 0.22546219881112362, 0.22453200373061902, 0.22360778716887186, 0.22268949304005609, 0.22177706598712182, 0.2208704513695468, 0.21996959525133525, 0.21907444438925983, 0.21818494622134005, 0.2173010488555526, 0.21642270105876724, 0.21554985224590445, 0.2146824524693079, 0.21382045240832953, 0.212963803359119, 0.2121124572246161, 0.21126636650473973, 0.2104254842867691, 0.20958976423591386, 0.20875916058606697, 0.207933628130739, 0.20711312221416722, 0.2062975987225966, 0.2054870140757296, 0.2046813252183397, 0.20388048961204563, 0.2030844652272432, 0.2022932105351904, 0.2015066845002432, 0.2007248465722378, 0.1999476566790177, 0.1991750752190995, 0.19840706305447936, 0.1976435815035706, 0.19688459233427527, 0.19613005775718412, 0.19537994041890216, 0.1946342033954981, 0.19389281018607352, 0.19315572470645165, 0.19242291128298086, 0.1916943346464506, 0.19096995992611976, 0.1902497526438519, 0.1895336787083573, 0.1888217044095385, 0.18811379641293818, 0.18740992175428528, 0.18671004783414008, 0.18601414241263392, 0.185322173604302, 0.18463410987300827, 0.1839499200269596, 0.18326957321380816, 0.182593038915839, 0.18192028694524212, 0.1812512874394676, 0.18058601085666035, 0.17992442797117447, 0.1792665098691657, 0.17861222794425874, 0.17796155389328971, 0.1773144597121216, 0.1766709176915302, 0.1760309004131611, 0.17539438074555452, 0.17476133184023793, 0.17413172712788422, 0.1735055403145342, 0.17288274537788317, 0.17226331656362867, 0.17164722838187987, 0.17103445560362557, 0.1704249732572618, 0.16981875662517565, 0.1692157812403864, 0.16861602288324073, 0.16801945757816256, 0.16742606159045614, 0.16683581142315956, 0.16624868381395178, 0.16566465573210729, 0.16508370437550118, 0.16450580716766194, 0.16393094175487086, 0.16335908600330906, 0.1627902179962487, 0.16222431603128898, 0.16166135861763647, 0.16110132447342768, 0.16054419252309407, 0.1599899418947683, 0.15943855191773115, 0.1588900021198974, 0.15834427222534236, 0.15780134215186478, 0.15726119200858962, 0.15672380209360529, 0.1561891528916394, 0.15565722507176832, 0.15512799948516282, 0.1546014571628677, 0.154077579313615, 0.15355634732167006, 0.15303774274471071, 0.15252174731173807, 0.1520083429210179, 0.15149751163805436, 0.15098923569359224, 0.1504834974816504, 0.14998027955758275, 0.14947956463616946, 0.14898133558973564, 0.1484855754462968, 0.1479922673877333, 0.14750139474798948, 0.1470129410113007, 0.14652688981044518, 0.14604322492502117, 0.14556193027974929, 0.1450829899427994, 0.14460638812414126, 0.14413210917391825, 0.143660137580846, 0.1431904579706314, 0.14272305510441652, 0.14225791387724246, 0.14179501931653643, 0.14133435658061896, 0.140875910957233, 0.140419667862093, 0.1399656128374542, 0.13951373155070218, 0.13906400979296205, 0.13861643347772615, 0.1381709886395019, 0.13772766143247667, 0.13728643812920263, 0.13684730511929816, 0.13641024890816822, 0.13597525611574093, 0.13554231347522233, 0.13511140783186731, 0.1346825261417674, 0.13425565547065516, 0.1338307829927235, 0.13340789598946254, 0.13298698184851024, 0.13256802806251972, 0.13215102222804062, 0.13173595204441602, 0.13132280531269303, 0.13091156993454908, 0.13050223391123056, 0.13009478534250724, 0.1296892124256387, 0.12928550345435505, 0.12888364681785108, 0.12848363099979287, 0.1280854445773375, 0.12768907622016568, 0.12729451468952596, 0.12690174883729222, 0.12651076760503285, 0.126121560023091, 0.12573411520967787, 0.12534842236997712, 0.12496447079525896, 0.12458224986200868, 0.12420174903106274, 0.12382295784675808, 0.12344586593609091, 0.1230704630078866, 0.12269673885197926, 0.12232468333840234, 0.12195428641658829, 0.12158553811457942, 0.12121842853824694, 0.1208529478705209, 0.12048908637062872, 0.12012683437334357, 0.11976618228824182, 0.1194071205989691, 0.11904963986251613, 0.11869373070850218, 0.11833938383846838, 0.11798659002517875, 0.11763534011192964, 0.1172856250118681, 0.11693743570731774, 0.116590763249113, 0.1162455987559411, 0.11590193341369193, 0.11555975847481573, 0.11521906525768832, 0.1148798451459838, 0.11454208958805398, 0.11420579009631687, 0.11387093824665004, 0.11353752567779263, 0.11320554409075417, 0.1128749852482295, 0.11254584097402115, 0.112218103152468, 0.11189176372788126, 0.11156681470398576, 0.11124324814336875, 0.11092105616693419, 0.11060023095336416, 0.11028076473858539, 0.10996264981524265, 0.1096458785321776, 0.10933044329391406, 0.10901633656014881, 0.1087035508452476, 0.10839207871774822, 0.10808191279986727, 0.10777304576701432, 0.10746547034730997, 0.10715917932111049, 0.10685416552053661, 0.10655042182900865, 0.10624794118078579, 0.1059467165605113, 0.10564674100276183, 0.10534800759160295, 0.10505050946014774, 0.10475423979012238, 0.10445919181143462, 0.10416535880174782, 0.10387273408605985, 0.10358131103628573, 0.10329108307084528, 0.10300204365425575, 0.10271418629672743, 0.10242750455376529, 0.10214199202577337, 0.10185764235766434, 0.10157444923847334, 0.10129240640097457, 0.10101150762130413, 0.1007317467185849, 0.10045311755455652, 0.1001756140332086, 0.09989923010041837, 0.09962395974359198, 0.09934979699130908, 0.09907673591297189, 0.09880477061845727, 0.098533895257773, 0.09826410402071717, 0.09799539113654117, 0.09772775087361642, 0.09746117753910441, 0.09719566547863015, 0.0969312090759587, 0.09666780275267571, 0.09640544096787049, 0.09614411821782276, 0.09588382903569248, 0.09562456799121292, 0.09536632969038658, 0.09510910877518441, 0.09485289992324841, 0.0945976978475965, 0.09434349729633068, 0.09409029305234853, 0.09383807993305718, 0.09358685279008998, 0.09333660650902656, 0.0930873360091152, 0.09283903624299825, 0.09259170219644017, 0.0923453288880583, 0.0920999113690561, 0.09185544472295931, 0.09161192406535486, 0.09136934454363177, 0.09112770133672493, 0.09088698965486153, 0.09064720473930976, 0.09040834186213002, 0.09017039632592871, 0.08993336346361408, 0.08969723863815465, 0.08946201724234001, 0.089227694698544, 0.08899426645849012, 0.08876172800301863, 0.08853007484185696, 0.08829930251339166, 0.08806940658444265, 0.0878403826500391, 0.08761222633319893, 0.08738493328470884, 0.08715849918290691, 0.08693291973346788, 0.0867081906691898, 0.0864843077497826, 0.08626126676165935, 0.08603906351772894, 0.08581769385719065, 0.0855971536453313, 0.08537743877332335, 0.08515854515802548, 0.08494046874178529, 0.0847232054922432, 0.08450675140213837, 0.08429110248911678, 0.08407625479554057, 0.08386220438829974, 0.08364894735862476, 0.0834364798219017, 0.0832247979174888, 0.08301389780853452, 0.08280377568179745, 0.08259442774746796, 0.08238585023899102, 0.08217803941289141, 0.08197099154859985, 0.08176470294828087, 0.08155916993666243, 0.08135438886086692, 0.08115035609024396, 0.08094706801620388, 0.08074452105205418, 0.08054271163283576, 0.08034163621516185, 0.08014129127705794, 0.07994167331780329, 0.0797427788577734, 0.07954460443828461, 0.07934714662143957, 0.07915040198997417, 0.07895436714710569, 0.07875903871638293, 0.07856441334153691, 0.07837048768633327, 0.07817725843442591, 0.07798472228921186, 0.07779287597368745, 0.07760171623030576, 0.07741123982083561, 0.07722144352622086, 0.07703232414644234, 0.07684387850037949, 0.0766561034256743, 0.07646899577859619, 0.07628255243390747, 0.07609677028473091, 0.07591164624241753, 0.0757271772364164, 0.07554336021414476, 0.07536019214085954, 0.0751776699995308, 0.0749957907907145, 0.0748145515324282, 0.07463394926002645, 0.07445398102607816, 0.07427464390024452, 0.07409593496915795, 0.07391785133630244, 0.07374039012189451, 0.07356354846276544, 0.07338732351224445, 0.07321171244004243, 0.07303671243213775, 0.07286232069066145, 0.07268853443378506, 0.07251535089560798, 0.07234276732604675, 0.07217078099072452, 0.07199938917086227, 0.07182858916317002, 0.07165837827973948, 0.0714887538479379, 0.07131971321030174, 0.0711512537244324, 0.07098337276289216, 0.07081606771310092, 0.07064933597723427, 0.07048317497212209, 0.0703175821291476, 0.07015255489414815, 0.06998809072731599, 0.06982418710310039, 0.06966084151011029, 0.06949805145101769, 0.06933581444246184, 0.06917412801495497, 0.06901298971278715, 0.06885239709393359, 0.06869234772996191, 0.06853283920593987, 0.06837386912034472, 0.06821543508497235, 0.06805753472484796, 0.06790016567813677, 0.06774332559605603, 0.06758701214278716, 0.06743122299538948, 0.06727595584371346, 0.06712120839031535, 0.06696697835037291, 0.06681326345160049, 0.06666006143416618, 0.06650737005060901, 0.06635518706575663, 0.06620351025664378, 0.06605233741243156, 0.06590166633432754, 0.06575149483550552, 0.06560182074102724, 0.06545264188776384, 0.06530395612431802, 0.065155761310947, 0.06500805531948634, 0.06486083603327354, 0.06471410134707337, 0.06456784916700234, 0.06442207741045557, 0.06427678400603215, 0.06413196689346301, 0.0639876240235383, 0.06384375335803509, 0.0637003528696467, 0.06355742054191163, 0.06341495436914318, 0.06327295235636016, 0.06313141251921756, 0.062990332883938, 0.0628497114872435, 0.06270954637628827, 0.06256983560859121, 0.06243057725196978, 0.06229176938447387, 0.06215341009432021, 0.062015497479827264, 0.061878029649351164, 0.06174100472122099, 0.06160442082367575, 0.06146827609480118, 0.06133256868246702, 0.06119729674426498, 0.06106245844744724, 0.060928051968864944, 0.060794075494907805, 0.060660527221443473, 0.0605274053537581, 0.06039470810649667, 0.060262433703604276, 0.06013058037826746, 0.05999914637285628, 0.05986812993886652, 0.05973752933686277, 0.059607342836421544, 0.05947756871607466, 0.05934820526325382, 0.059219250774234615, 0.059090703554081865, 0.05896256191659431, 0.058834824184251205, 0.05870748868815742, 0.05858055376799075, 0.058454017771948294, 0.05832787905669411, 0.058202135987306544, 0.05807678693722652, 0.05795183028820589, 0.05782726443025624, 0.05770308776159819, 0.057579298688610614, 0.057455895625780906, 0.0573328769956552, 0.05721024122878886, 0.057087986763697515, 0.05696611204680864, 0.05684461553241284, 0.0567234956826164, 0.05660275096729327, 0.056482379864038035, 0.05636238085811893, 0.05624275244243121, 0.056123493117451025, 0.05600460139118947, 0.055886075779146775, 0.055767914804267515, 0.05565011699689519, 0.05553268089472778, 0.05541560504277368, 0.055298887993307275, 0.055182528305825734, 0.05506652454700517, 0.054950875290658056, 0.05483557911769006, 0.05472063461605796, 0.05460604038072726, 0.054491795013630356, 0.05437789712362502, 0.05426434532645334, 0.05415113824470024, 0.05403827450775334, 0.05392575275176223, 0.053813571619598516, 0.05370172976081586, 0.05359022583161052, 0.05347905849478191, 0.053368226419693854, 0.05325772828223568, 0.05314756276478373, 0.05303772855616309, 0.05292822435161008, 0.05281904885273395, 0.05271020076748004, 0.05260167881009238, 0.05249348170107649, 0.05238560816716345, 0.0522780569412729, 0.05217082676247712, 0.052063916375965115, 0.05195732453300722, 0.05185104999091911, 0.051745091513027475, 0.051639447868634114, 0.051534117832982285, 0.05142910018722137, 0.05132439371837336, 0.05121999721929853, 0.05111590948866164, 0.05101212933089867, 0.05090865555618349, 0.050805486980394554, 0.05070262242508251, 0.05060006071743699, 0.050497800690254904, 0.05039584118190772, 0.050294181036309824, 0.0501928191028866, 0.050091754236543024, 0.049990985297632407, 0.049890511151925096, 0.04979033067057772, 0.04969044273010241, 0.04959084621233645, 0.049491540004412034, 0.04939252299872594, 0.049293794092909834, 0.049195352189800604, 0.04909719619741089, 0.048999325028899615, 0.04890173760254335, 0.04880443284170688, 0.04870740967481478, 0.048610667035322935, 0.04851420386169012, 0.04841801909735001, 0.04832211169068287, 0.04822648059498838, 0.04813112476845759, 0.04803604317414544, 0.04794123477994407, 0.047846698558555376, 0.047752433487464156, 0.047658438548911504, 0.04756471272986845, 0.04747125502200934, 0.04737806442168579, 0.04728513992990084, 0.047192480552282805, 0.04710008529905996, 0.047007953185034854, 0.04691608322955894, 0.04682447445650763, 0.04673312589425511, 0.046642036575649554, 0.04655120553798843, 0.046460631822994175, 0.0463703144767895, 0.04628025254987353, 0.04619044509709741, 0.04610089117764081, 0.04601158985498791, 0.045922540196903866, 0.045833741275411524, 0.04574519216676791, 0.04565689195144137, 0.04556883971408825, 0.045481034543530516, 0.04539347553273264, 0.045306161778779094, 0.04521909238285234, 0.045132266450210165, 0.04504568309016378, 0.04495934141605562, 0.044873240545237864, 0.04478737959905045, 0.044701757702799556, 0.04461637398573631, 0.04453122758103527, 0.044446317625773585, 0.04436164326090983, 0.044277203631262814, 0.044192997885491525, 0.04410902517607363, 0.04402528465928586, 0.043941775495182994, 0.04385849684757784, 0.04377544788402127, 0.04369262777578205, 0.04361003569782687, 0.04352767082880108, 0.04344553235100856, 0.043363619450392334, 0.04328193131651556, 0.043200467142541984, 0.04311922612521668, 0.043038207464847336, 0.04295741036528531, 0.04287683403390655, 0.042796477681593524, 0.042716340522716015, 0.04263642177511326, 0.042556720660075195, 0.042477236402324495, 0.04239796822999857, 0.04231891537463124, 0.04224007707113536, 0.04216145255778448, 0.042083041076195746, 0.04200484187131208, 0.0419268541913846]\n",
      "Final weights of Perceptron are: \n",
      "\n",
      "W0:  -0.9277447198109581 \n",
      "\n",
      "W1:  0.9511600082912268 \n",
      "\n",
      "W2:  0.9498108559260638 \n",
      "\n",
      "The number of epochs are:  390\n",
      "The error counts of the ReLU step function perceptron are:  [300.16353281249997, 170.39313595313018, 102.7652019210916, 66.98001022150997, 47.581816689810616, 36.1132934413367, 28.124551190200535, 22.42922681397694, 18.324039261549782, 15.326918241235493, 13.106524014223647, 11.43438253715373, 10.152350658272573, 9.150478061298854, 8.351924413585472, 7.6993524891463085, 7.144458856096855, 6.665104343584681, 6.247598902269077, 5.881102208764658, 5.556990719912581, 5.2683701517897, 5.0096993407228325, 4.7764994958788325, 4.565128989697716, 4.372608510676381, 4.196484970660254, 4.034725280590629, 3.8856331860717996, 3.7477839405325937, 3.6199728059192275, 3.5011742976715174, 3.390509799904562, 3.287221719812168, 3.1906527666161626, 3.1002292598492067, 3.0154476172031703, 2.9358633610065095, 2.861082127891126, 2.7907522785064174, 2.7244104158002473, 2.6606371013579926, 2.5983525477772242, 2.5377862695603164, 2.479118907007197, 2.422488007178168, 2.367993922014175, 2.3157054220723525, 2.2656648197209766, 2.217892516077542, 2.172390957813265, 2.129148030633902, 2.0881399372747866, 2.049333616739716, 2.0126887631497326, 1.9781595000802754, 1.945695761631495, 1.9152444259199208, 1.8867502409328383, 1.8601565771678072, 1.8354060364077793, 1.812440941453013, 1.735457333994404, 1.6439234201958777, 1.558220660260444, 1.4777570203588417, 1.4021538537654004, 1.331125253187027, 1.264424127746735, 1.2018197326298028, 1.1430895038387408, 1.0880170157305522, 1.0363922679113529, 0.9880126241905371, 0.9426837233387634, 0.9002201342406697, 0.8604457211129866, 0.8231937570550071, 0.7883068434971299, 0.7556366907908312, 0.7250438056421298, 0.6963971204817412, 0.6695735905097921, 0.6444577766751645, 0.6209414271826283, 0.5989230659659773, 0.5783075935871727, 0.5590059039229873, 0.5409345185432702, 0.5240152396868576, 0.5081748220681324, 0.49334466330200744, 0.4794605124483799, 0.46646219599973604, 0.4542933605331865, 0.4429012311966808, 0.4322363851817782, 0.42225253934054885, 0.4129063511239589, 0.4041572320479603, 0.395967172927732, 0.3883005801575393, 0.3811241223518185, 0.3743281831401294, 0.3676529688515875, 0.36107003125935405, 0.35458381228401725, 0.3481978590637238, 0.34191466095949985, 0.3357357254411393, 0.32966173722212483, 0.3236927294748835, 0.3178282378868267, 0.3120674283579393, 0.30640919811189, 0.30085225351411626, 0.295395168842507, 0.2900364300871907, 0.2847744672717828, 0.2796076781169806, 0.2745344452472993, 0.2695531486204771, 0.2646621744425597, 0.25985992150888837, 0.2551448056659587, 0.2505152629052246, 0.2459697514633043, 0.2415067532021976, 0.23712477446902813, 0.23282234658056877, 0.2285980260381718, 0.22445039454982485, 0.22037805891500828, 0.2163796508127219, 0.21245382652191597, 0.20859926659548503, 0.20481467550310395, 0.20109878125393676, 0.19745033500714965, 0.19386811067592555, 0.19035090452905218, 0.18689753479297866, 0.18350684125638606, 0.18017768487870248, 0.17690894740354662, 0.1736995309777658, 0.17054835777650268, 0.1674543696345587, 0.164416527684204, 0.16143381199950121, 0.15850522124714683, 0.1556297723437962, 0.15280650011980534, 0.15003445698930548, 0.1473127126265118, 0.1446403536481598, 0.14201648330195726, 0.13944022116093752, 0.13691070282359852, 0.13442707961971237, 0.1319885183216915, 0.12959420086139883, 0.12724332405228989, 0.12493509931678179, 0.12266875241873863, 0.12044352320097318, 0.11825866532766131, 0.11611344603157112, 0.11400714586601035, 0.11193905846139675, 0.10990849028636078, 0.10791476041328965, 0.1059572002882253, 0.10403515350503179, 0.10214797558374605, 0.10029503375303273, 0.09847570673666235, 0.09668938454393378, 0.0949354682639666, 0.09321336986378764, 0.09152251199013825, 0.08986232777493303, 0.08823226064429773, 0.0866317641311203, 0.08506030169104606, 0.08351734652185383, 0.08200238138614768, 0.08051489843730168, 0.07905439904859649, 0.07762039364548831, 0.07621240154094988, 0.07482995077382737, 0.07347257795015597, 0.07213982808737834, 0.07083125446141286, 0.06954641845651754, 0.06828488941789804, 0.06704624450700845, 0.06583006855949451, 0.06463595394573163, 0.06346350043390697, 0.06231231505560196, 0.06118201197382494, 0.06007221235345224, 0.0589825442340311, 0.057912642404901335, 0.05686214828259317, 0.055830709790458996, 0.054817981240498165, 0.05382362321733448, 0.052847302464305974, 0.051888691771629976, 0.050947469866602875, 0.05002332130579959, 0.0491159363692342, 0.04822501095644697, 0.04735024648448201, 0.046491349787720807, 0.04564803301953745, 0.04482001355574337, 0.044007013899787076, 0.04320876158967834, 0.04242498910660389, 0.04165543378520552, 0.040899837725488136, 0.04015794770632947, 0.039429515100561835, 0.03871429579159667, 0.03801205009156447, 0.03732254266094233, 0.036645542429640965, 0.03598082251952647, 0.03532816016834876, 0.03468733665505203, 0.0340581372264423, 0.033440351025186194, 0.03283377101911829, 0.032238193931831305, 0.03165342017452755, 0.03107925377910766, 0.03051550233247408, 0.029961976912027947, 0.029418492022336484, 0.028884865532950953, 0.028360918617352876, 0.027846475693009045, 0.027341364362514935, 0.026845415355805882, 0.026358462473418054, 0.02588034253077885, 0.025410895303508624, 0.024949963473715187, 0.024497392577263473, 0.024053030952001744, 0.02361672968692754, 0.02318834257227666, 0.022767726050517433, 0.022354739168234647, 0.0219492435288866, 0.021551103246419505, 0.021160184899723513, 0.020776357487915734, 0.02039949238643388, 0.020029463303927235, 0.019666146239929974, 0.019309419443301883, 0.018959163371423893, 0.018615260650133483, 0.018277596034387337, 0.017946056369637704, 0.01762053055390946, 0.017300909500565415, 0.01698708610174686, 0.016678955192478077, 0.016376413515421354, 0.016079359686271912, 0.01578769415978078, 0.015501319196393557, 0.015220138829495189, 0.014944058833248326, 0.014672986691015646, 0.014406831564354714, 0.014145504262575805, 0.013888917212851345, 0.013636984430868358, 0.013389621492012749, 0.013146745503076759, 0.012908275074479225, 0.012674130292990464, 0.012444232694951318, 0.012218505239978392, 0.011996872285146336, 0.011779259559638188, 0.011565594139856004, 0.011355804424982569, 0.011149820112986873, 0.010947572177064517, 0.010748992842505588, 0.010554015563982316, 0.010362575003248707, 0.010174607007244502, 0.009990048586596549, 0.00980883789451014, 0.009630914206043222, 0.009456217897756601, 0.00928469042773333, 0.009116274315960385, 0.008950913125066587, 0.008788551441409356, 0.008629134856505266, 0.008472609948796821, 0.008318924265750347, 0.00816802630627846, 0.008019865503481566, 0.007874392207702212, 0.007731557669886913, 0.007591314025249916, 0.007453614277233124, 0.007318412281756984, 0.007185662731757295, 0.007055321142002231, 0.00692734383418483, 0.006801687922286142, 0.006678311298203598, 0.00655717261764016, 0.006438231286249471, 0.006321447446032464, 0.00620678196198025, 0.00609419640896003, 0.005983653058838124, 0.005875114867837221, 0.005768545464122511, 0.005663909135613264, 0.005561170818015355, 0.005460296083070986, 0.005361251127021434, 0.005264002759279142, 0.005168518391305351, 0.005074766025689334, 0.0049827142454258715, 0.004892332203387153, 0.004803589611985595, 0.004716456733024357, 0.004630904367731767, 0.004546903846976648, 0.00446442702166097, 0.004383446253287007, 0.0043039344046953715, 0.004225864830971256, 0.004149211370515413, 0.004073948336277259, 0.004000050507147025, 0.003927493119503886, 0.0038562518589175714, 0.0037863028520003315, 0.003717622658406961, 0.0036501882629795556, 0.0035839770680351625, 0.0035189668857930827, 0.003455135930939545, 0.003392462813327304, 0.0033309265308077963, 0.003270506462192956, 0.00321118236034508, 0.0031529343453918137, 0.0030957428980642942, 0.0030395888531560746, 0.002984453393100804, 0.0029303180416662706, 0.0028771646577628934, 0.0028249754293646033, 0.0027737328675398057, 0.0027234198005908665, 0.002674019368299562, 0.002625515016277331, 0.0025778904904176217, 0.0025311298314491517, 0.002485217369587756, 0.0024401377192854335, 0.002395875774074447, 0.002352416701505173, 0.0023097459381755643, 0.00226784918485086, 0.0022267124016718816, 0.0021863218034501196, 0.0021466638550482592, 0.0021077252668445026, 0.002069492990279079, 0.0020319542134816125, 0.0019950963569777946]\n"
     ]
    }
   ],
   "source": [
    "# The parameters being defined\n",
    "weight0 = 10\n",
    "weight1 = 0.2\n",
    "weight2 = -0.75\n",
    "learning_rate = 0.05\n",
    "target_output_and = [0, 0, 0, 1]\n",
    "\n",
    "# Finding the error counts of the bipolar activation function perceptron\n",
    "epochs_count_bipolar, error_count_bipolar = perceptron(data_gates_input, target_output_and, weight0, weight1, weight2, learning_rate, 'bipolar')\n",
    "print('The number of epochs are: ', epochs_count_bipolar)\n",
    "print('The error counts of the bipolar step function perceptron are: ', error_count_bipolar)\n",
    "\n",
    "# Finding the error counts of the sigmoid activation function perceptron\n",
    "epochs_count_sigmoid, error_count_sigmoid = perceptron(data_gates_input, target_output_and, weight0, weight1, weight2, learning_rate, 'sigmoid')\n",
    "print('The number of epochs are: ', epochs_count_sigmoid)\n",
    "print('The error counts of the sigmoid step function perceptron are: ', error_count_sigmoid)\n",
    "\n",
    "# Finding the error counts of the ReLU activation function perceptron\n",
    "epochs_count_relu, error_count_relu = perceptron(data_gates_input, target_output_and, weight0, weight1, weight2, learning_rate, 'relu')\n",
    "print('The number of epochs are: ', epochs_count_relu)\n",
    "print('The error counts of the ReLU step function perceptron are: ', error_count_relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final weights of Perceptron are: \n",
      "\n",
      "W0:  -0.2999999999999812 \n",
      "\n",
      "W1:  0.19999999999999998 \n",
      "\n",
      "W2:  0.15000000000000005 \n",
      "\n",
      "Final weights of Perceptron are: \n",
      "\n",
      "W0:  -0.599999999999998 \n",
      "\n",
      "W1:  0.39999999999999997 \n",
      "\n",
      "W2:  0.24999999999999978 \n",
      "\n",
      "Final weights of Perceptron are: \n",
      "\n",
      "W0:  -0.2000000000000009 \n",
      "\n",
      "W1:  0.2 \n",
      "\n",
      "W2:  0.1500000000000002 \n",
      "\n",
      "Final weights of Perceptron are: \n",
      "\n",
      "W0:  -1.2000000000000044 \n",
      "\n",
      "W1:  1.0000000000000002 \n",
      "\n",
      "W2:  0.4500000000000002 \n",
      "\n",
      "Final weights of Perceptron are: \n",
      "\n",
      "W0:  -1.5 \n",
      "\n",
      "W1:  1.2 \n",
      "\n",
      "W2:  0.75 \n",
      "\n",
      "Final weights of Perceptron are: \n",
      "\n",
      "W0:  -1.999999999999997 \n",
      "\n",
      "W1:  1.4 \n",
      "\n",
      "W2:  1.0499999999999998 \n",
      "\n",
      "Final weights of Perceptron are: \n",
      "\n",
      "W0:  -1.9 \n",
      "\n",
      "W1:  1.5999999999999999 \n",
      "\n",
      "W2:  0.6500000000000004 \n",
      "\n",
      "Final weights of Perceptron are: \n",
      "\n",
      "W0:  -2.0000000000000004 \n",
      "\n",
      "W1:  1.8 \n",
      "\n",
      "W2:  0.8499999999999996 \n",
      "\n",
      "Final weights of Perceptron are: \n",
      "\n",
      "W0:  -2.600000000000002 \n",
      "\n",
      "W1:  2.0 \n",
      "\n",
      "W2:  1.0500000000000003 \n",
      "\n",
      "Final weights of Perceptron are: \n",
      "\n",
      "W0:  -3 \n",
      "\n",
      "W1:  2.2 \n",
      "\n",
      "W2:  1.25 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Question A3:\n",
    "# With changing learning rates and apply step activation function perceptron\n",
    "\n",
    "weight0 = 10\n",
    "weight1 = 0.2\n",
    "weight2 = -0.75\n",
    "learning_rates = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "target_output_and = [0, 0, 0, 1]\n",
    "epochs_count_learning_and = []\n",
    "# Applying the activation function\n",
    "for i in learning_rates:\n",
    "    epochs_count_step_and, error_count_step_and = perceptron(data_gates_input, target_output_and, weight0, weight1, weight2, i, 'step')\n",
    "    epochs_count_learning_and.append(epochs_count_step_and)\n",
    "\n",
    "# Plotting epochs counts against learning rates\n",
    "plt.plot(epochs_count_learning_and, learning_rates, color = 'red')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Learning rates')\n",
    "plt.title('Epoch count vs Learning rate for AND logic')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final weights of Perceptron are: \n",
      "\n",
      "W0:  0.09999999999999236 \n",
      "\n",
      "W1:  -0.09999999999999969 \n",
      "\n",
      "W2:  -0.09999999999999969 \n",
      "\n",
      "The number of epochs are:  1000\n",
      "The error counts of the step activation function perceptron are:  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 2, 1, 2, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Question A4:\n",
    "# Repeating the questions A1 - A3 for XOR logic\n",
    "weight0 = 10\n",
    "weight1 = 0.2\n",
    "weight2 = -0.75\n",
    "learning_rate = 0.05\n",
    "\n",
    "data_gates_input = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "target_output_xor = [0, 1, 1, 0]\n",
    "\n",
    "# Question A1 for XOR logic\n",
    "epochs_count_step, error_count_step = perceptron(data_gates_input, target_output_xor, weight0, weight1, weight2, learning_rate, 'step')\n",
    "print('The number of epochs are: ', epochs_count_step)\n",
    "print('The error counts of the step activation function perceptron are: ', error_count_step)\n",
    "\n",
    "# Plotting the epochs count vs error counts for XOR logic\n",
    "\n",
    "plt.plot(range(epochs_count_step), error_count_step, color='blue')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Error values')\n",
    "plt.title('Epoch count vs Error value for XOR logic')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final weights of Perceptron are: \n",
      "\n",
      "W0:  0.1999999999999919 \n",
      "\n",
      "W1:  -0.09999999999999969 \n",
      "\n",
      "W2:  -0.15000000000000013 \n",
      "\n",
      "The number of epochs are:  1000\n",
      "The error counts of the bipolar step function perceptron are:  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 6, 2, 2, 2, 6, 2, 2, 2, 6, 2, 2, 2, 6, 2, 2, 2, 6, 2, 2, 2, 6, 2, 2, 2, 6, 2, 2, 2, 6, 2, 2, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "Final weights of Perceptron are: \n",
      "\n",
      "W0:  0.02876922691700229 \n",
      "\n",
      "W1:  -0.05348672816495928 \n",
      "\n",
      "W2:  -0.02819591018380748 \n",
      "\n",
      "The number of epochs are:  1000\n",
      "The error counts of the sigmoid step function perceptron are:  [1.9997437931464885, 1.9996976367803057, 1.9996423775043994, 1.9995761194702242, 1.9994965606599728, 1.9994009048562478, 1.9992857545418952, 1.9991469806566387, 1.9989795643005874, 1.9987774044832878, 1.9985330848508014, 1.9982375909645682, 1.9978799681426969, 1.9974469081063324, 1.9969222507208635, 1.9962863850375048, 1.9955155317325053, 1.9945808870986004, 1.9934476072798595, 1.9920736109513761, 1.9904081798819742, 1.9883903408923747, 1.9859470212242065, 1.9829909844740121, 1.9794185789683971, 1.9751073685102365, 1.9699137712573158, 1.9636709108096302, 1.95618698830779, 1.9472446166367248, 1.9366017127024122, 1.9239947050768067, 1.9091449484113538, 1.8917692853292074, 1.871595576832836, 1.8483836301130956, 1.8219511911061104, 1.7922034992223272, 1.7591634157194898, 1.7229976292645204, 1.6840334225501405, 1.6427605725177945, 1.5998146286310275, 1.5559410667298168, 1.511943954934422, 1.4686265193676145, 1.426733011425604, 1.386900787047932, 1.3496287316420925, 1.315264185064322, 1.28400671883308, 1.2559245373152592, 1.2309782903427158, 1.2090474579384813, 1.1899556424315165, 1.1734925134767407, 1.1594314034329307, 1.1475424609755076, 1.1376018184939254, 1.1293974806168017, 1.1227326914212288, 1.1174274726165438, 1.1133189080162986, 1.1102606213456307, 1.108121776718474, 1.106785833239191, 1.106149208985828, 1.1061199531882435, 1.1066164852982883, 1.107566432205924, 1.1089055768056335, 1.110576919778385, 1.1125298497550555, 1.1147194134355296, 1.1171056756570565, 1.119653159072973, 1.1223303535016294, 1.1251092857947531, 1.1279651420367687, 1.1308759348882793, 1.1338222098503339, 1.1367867851102678, 1.1397545204167878, 1.142712111117637, 1.1456479040817233, 1.1485517327274435, 1.1514147688000675, 1.1542293888941604, 1.1569890540119907, 1.159688200694887, 1.1623221424696148, 1.1648869805229725, 1.167379522660852, 1.1697972097279197, 1.172138048764946, 1.1744005522660665, 1.176583682970614, 1.1786868036859635, 1.1807096316909014, 1.182652197314851, 1.1845148063281445, 1.186298005813366, 1.1880025532184433, 1.1896293883192781, 1.1911796078438375, 1.1926544425311798, 1.194055236418283, 1.1953834281649742, 1.1966405342431203, 1.1978281338305612, 1.1989478552633728, 1.2000013639119858, 1.2009903513576141, 1.201916525755457, 1.2027816032803507, 1.2035873005589766, 1.2043353280005062, 1.2050273839447367, 1.205665149553327, 1.2062502843758491, 1.20678442252793, 1.207269169423954, 1.2077060990115092, 1.2080967514591796, 1.2084426312532994, 1.2087452056630204, 1.209005903536466, 1.2092261143939165, 1.2094071877868608, 1.2095504328944562, 1.2096571183313714, 1.2097284721432864, 1.2097656819683944, 1.2097698953451679, 1.2097422201484225, 1.2096837251373365, 1.2095954406005487, 1.2094783590848617, 1.2093334361952899, 1.209161591455377, 1.2089637092177374, 1.2087406396157574, 1.208493199548272, 1.2082221736898409, 1.2079283155199951, 1.2076123483655028, 1.2072749664503217, 1.2069168359484712, 1.2065385960355837, 1.2061408599353565, 1.2057242159575645, 1.205289228524688, 1.2048364391845594, 1.2043663676067695, 1.2038795125608697, 1.2033763528746635, 1.2028573483711487, 1.2023229407828708, 1.2017735546426642, 1.201209598149941, 1.200631464011843, 1.2000395302587288, 1.1994341610336046, 1.198815707355223, 1.198184507854689, 1.1975408894855006, 1.1968851682070614, 1.1962176496417523, 1.1955386297057418, 1.1948483952137683, 1.1941472244581885, 1.1934353877626211, 1.192713148010582, 1.1919807611495188, 1.1912384766707105, 1.190486538065502, 1.1897251832583948, 1.1889546450175073, 1.1881751513429617, 1.1873869258337537, 1.1865901880336733, 1.1857851537568678, 1.1849720353936257, 1.1841510421969843, 1.1833223805507507, 1.182486254219541, 1.181642864581434, 1.1807924108438401, 1.1799350902431738, 1.1790710982289314, 1.1782006286327544, 1.1773238738230634, 1.1764410248458366, 1.1755522715521, 1.174657802712699, 1.1737578061208913, 1.1728524686833142, 1.1719419764998578, 1.1710265149329748, 1.1701062686669315, 1.1691814217575163, 1.1682521576726947, 1.1673186593246971, 1.1663811090940084, 1.1654396888457275, 1.1644945799387432, 1.1635459632281664, 1.1625940190614454, 1.161638927268585, 1.1606808671468674, 1.1597200174404712, 1.158756556315373, 1.1577906613298938, 1.1568225094012534, 1.1558522767684767, 1.15488013895199, 1.1539062707102237, 1.152930845993538, 1.151954037895775, 1.1509760186037123, 1.149996959344712, 1.1490170303328235, 1.1480364007135928, 1.147055238507827, 1.146073710554549, 1.1450919824533616, 1.1441102185064327, 1.14312858166031, 1.142147233447751, 1.141166333929752, 1.140186041637951, 1.1392065135175626, 1.1382279048710024, 1.13725036930234, 1.1362740586627196, 1.1352991229968734, 1.1343257104908402, 1.133353967421004, 1.1323840381045513, 1.1314160648514369, 1.130450187917944, 1.1294865454619216, 1.1285252734997562, 1.1275665058651545, 1.1266103741697797, 1.1256570077658035, 1.124706533710403, 1.1237590767322505, 1.1228147592000177, 1.121873701092925, 1.1209360199733476, 1.120001830961506, 1.119071246712232, 1.118144377393824, 1.117221330668995, 1.1163022116778956, 1.1153871230232162, 1.1144761647573402, 1.1135694343715514, 1.1126670267872525, 1.1117690343491877, 1.1108755468206377, 1.1099866513805519, 1.1091024326225911, 1.1082229725560513, 1.1073483506086126, 1.106478643630895, 1.105613925902768, 1.10475426914137, 1.1038997425108033, 1.1030504126334415, 1.1022063436028202, 1.101367596998051, 1.1005342318997093, 1.0997063049071545, 1.0988838701572168, 1.0980669793442164, 1.097255681741249, 1.0964500242226909, 1.0956500512878713, 1.0948558050858594, 1.0940673254413094, 1.0932846498813187, 1.092507813663239, 1.0917368498033924, 1.090971789106642, 1.0902126601967619, 1.0894594895475593, 1.0887123015146996, 1.0879711183681775, 1.0872359603253994, 1.0865068455848146, 1.085783790360059, 1.0850668089145592, 1.0843559135965566, 1.083651114874502, 1.082952421372784, 1.0822598399077408, 1.0815733755239274, 1.0808930315305816, 1.0802188095382643, 1.07955070949563, 1.078888729726291, 1.0782328669657422, 1.0775831163983094, 1.0769394716940937, 1.0763019250458703, 1.0756704672059243, 1.0750450875227808, 1.0744257739778118, 1.0738125132216838, 1.073205290610629, 1.0726040902425058, 1.072008894992636, 1.071419686549383, 1.0708364454494614, 1.0702591511129516, 1.0696877818780008, 1.0691223150351927, 1.068562726861568, 1.0680089926542817, 1.0674610867638792, 1.0669189826271785, 1.0663826527997462, 1.06585206898795, 1.0653272020805857, 1.0648080221800575, 1.0642944986331095, 1.0637866000610965, 1.0632842943897862, 1.0627875488786862, 1.0622963301498891, 1.0618106042164295, 1.061330336510149, 1.0608554919090625, 1.0603860347642269, 1.0599219289261015, 1.0594631377704071, 1.0590096242234721, 1.0585613507870737, 1.0581182795627657, 1.0576803722756989, 1.057247590297928, 1.0568198946712126, 1.056397246129308, 1.0559796051197528, 1.0555669318251457, 1.0551591861839287, 1.054756327910665, 1.0543583165158255, 1.053965111325076, 1.0535766714980874, 1.0531929560468485, 1.052813923853509, 1.0524395336877406, 1.052069744223631, 1.0517045140561117, 1.0513438017169237, 1.0509875656901309, 1.050635764427185, 1.0502883563615448, 1.0499452999228587, 1.049606553550719, 1.049272075707989, 1.0489418248937104, 1.0486157596556036, 1.048293838602157, 1.0479760204143214, 1.0476622638568105, 1.0473525277890172, 1.0470467711755513, 1.0467449530964021, 1.0464470327567437, 1.0461529694963736, 1.0458627227988069, 1.0455762523000218, 1.0452935177968732, 1.0450144792551683, 1.0447390968174266, 1.0444673308103172, 1.0441991417517897, 1.043934490357902, 1.0436733375493479, 1.0434156444577007, 1.0431613724313697, 1.0429104830412828, 1.0426629380862962, 1.0424186995983447, 1.0421777298473325, 1.0419399913457734, 1.0417054468531879, 1.041474059380262, 1.0412457921927727, 1.0410206088152893, 1.040798473034654, 1.040579348903248, 1.0403632007420507, 1.0401499931434954, 1.0399396909741274, 1.0397322593770726, 1.03952766377432, 1.0393258698688215, 1.0391268436464192, 1.038930551377602, 1.038736959619095, 1.0385460352152938, 1.0383577452995385, 1.038172057295241, 1.0379889389168662, 1.037808358170771, 1.0376302833559101, 1.0374546830644067, 1.0372815261819963, 1.0371107818883516, 1.0369424196572818, 1.0367764092568224, 1.0366127207492117, 1.0364513244907605, 1.0362921911316203, 1.0361352916154494, 1.0359805971789862, 1.0358280793515284, 1.0356777099543253, 1.0355294610998822, 1.0353833051911858, 1.03523921492085, 1.0350971632701857, 1.0349571235081978, 1.0348190691905141, 1.0346829741582475, 1.0345488125367936, 1.0344165587345688, 1.0342861874416889, 1.0341576736285927, 1.0340309925446152, 1.0339061197165043, 1.0337830309468967, 1.0336617023127428, 1.033542110163691, 1.0334242311204291, 1.0333080420729888, 1.0331935201790117, 1.033080642861981, 1.0329693878094206, 1.032859732971062, 1.032751656556985, 1.0326451370357275, 1.0325401531323724, 1.0324366838266101, 1.0323347083507781, 1.032234206187881, 1.032135157069591, 1.0320375409742306, 1.03194133812474, 1.0318465289866285, 1.0317530942659148, 1.0316610149070518, 1.0315702720908446, 1.0314808472323571, 1.031392721978808, 1.031305878207464, 1.0312202980235239, 1.031135963757997, 1.0310528579655818, 1.0309709634225346, 1.0308902631245436, 1.0308107402845952, 1.030732378330846, 1.0306551609044887, 1.0305790718576249, 1.0305040952511377, 1.030430215352567, 1.0303574166339893, 1.0302856837698988, 1.0302150016350997, 1.0301453553025968, 1.0300767300414964, 1.0300091113149148, 1.0299424847778897, 1.0298768362753041, 1.029812151839816, 1.0297484176897975, 1.0296856202272833, 1.029623746035928, 1.029562781878976, 1.029502714697239, 1.029443531607087, 1.0293852198984474, 1.0293277670328198, 1.0292711606412999, 1.029215388522616, 1.0291604386411788, 1.029106299125144, 1.0290529582644878, 1.0290004045090948, 1.0289486264668612, 1.0288976129018113, 1.0288473527322248, 1.0287978350287856, 1.0287490490127362, 1.0287009840540526, 1.0286536296696323, 1.0286069755214953, 1.0285610114150026, 1.0285157272970886, 1.0284711132545077, 1.0284271595120977, 1.0283838564310572, 1.0283411945072396, 1.0282991643694608, 1.0282577567778253, 1.0282169626220627, 1.028176772919887, 1.0281371788153641, 1.0280981715773008, 1.0280597425976448, 1.0280218833899046, 1.0279845855875815, 1.0279478409426193, 1.0279116413238687, 1.027875978715568, 1.0278408452158396, 1.0278062330351996, 1.0277721344950872, 1.0277385420264058, 1.0277054481680816, 1.0276728455656368, 1.027640726969778, 1.027609085235001, 1.027577913318211, 1.0275472042773544, 1.0275169512700704, 1.027487147552356, 1.0274577864772443, 1.0274288614935008, 1.0274003661443298, 1.0273722940661028, 1.0273446389870926, 1.0273173947262304, 1.027290555191871, 1.0272641143805767, 1.0272380663759122, 1.0272124053472576, 1.0271871255486296, 1.0271622213175233, 1.0271376870737638, 1.0271135173183716, 1.0270897066324436, 1.0270662496760476, 1.0270431411871273, 1.0270203759804244, 1.0269979489464118, 1.0269758550502401, 1.026954089330698, 1.0269326468991853, 1.0269115229386976, 1.026890712702825, 1.026870211514764, 1.0268500147663389, 1.0268301179170392, 1.026810516493066, 1.026791206086394, 1.026772182353842, 1.026753441016157, 1.0267349778571098, 1.0267167887226045, 1.0266988695197934, 1.026681216216211, 1.0266638248389133, 1.0266466914736332, 1.0266298122639403, 1.0266131834104204, 1.0265968011698576, 1.0265806618544335, 1.0265647618309317, 1.0265490975199572, 1.0265336653951642, 1.0265184619824934, 1.0265034838594207, 1.0264887276542167, 1.026474190045215, 1.0264598677600865, 1.0264457575751353, 1.0264318563145878, 1.0264181608499048, 1.0264046680990975, 1.0263913750260514, 1.026378278639862, 1.02636537599418, 1.0263526641865623, 1.0263401403578354, 1.0263278016914648, 1.0263156454129345, 1.0263036687891347, 1.0262918691277587, 1.0262802437767062, 1.026268790123497, 1.0262575055946916, 1.0262463876553194, 1.0262354338083166, 1.02622464159397, 1.0262140085893705, 1.0262035324078722, 1.0261932106985603, 1.0261830411457262, 1.0261730214683509, 1.0261631494195926, 1.0261534227862856, 1.0261438393884428, 1.0261343970787686, 1.0261250937421746, 1.0261159272953064, 1.0261068956860724, 1.026097996893185, 1.0260892289257022, 1.0260805898225824, 1.026072077652238, 1.0260636905121014, 1.026055426528195, 1.0260472838547061, 1.0260392606735722, 1.026031355194064, 1.0260235656523853, 1.026015890311269, 1.0260083274595846, 1.0260008754119485, 1.025993532508342, 1.0259862971137328, 1.0259791676177041, 1.0259721424340875, 1.0259652200006, 1.0259583987784906, 1.025951677252187, 1.025945053928951, 1.0259385273385366, 1.0259320960328553, 1.0259257585856436, 1.0259195135921386, 1.0259133596687535, 1.0259072954527648, 1.0259013196019962, 1.0258954307945132, 1.0258896277283203, 1.0258839091210599, 1.0258782737097203, 1.0258727202503446, 1.025867247517746, 1.0258618543052231, 1.0258565394242876, 1.0258513017043853, 1.0258461399926313, 1.0258410531535411, 1.025836040068772, 1.025831099636863, 1.0258262307729828, 1.025821432408679, 1.0258167034916308, 1.0258120429854072, 1.0258074498692278, 1.025802923137727, 1.0257984618007217, 1.0257940648829824, 1.0257897314240085, 1.025785460477805, 1.0257812511126654, 1.0257771024109545, 1.025773013468898, 1.0257689833963721, 1.0257650113166974, 1.025761096366437, 1.025757237695195, 1.0257534344654207, 1.025749685852215, 1.0257459910431375, 1.0257423492380204, 1.0257387596487801, 1.025735221499239, 1.0257317340249401, 1.025728296472974, 1.0257249081018012, 1.025721568181083, 1.0257182759915098, 1.0257150308246348, 1.0257118319827094, 1.0257086787785232, 1.0257055705352405, 1.0257025065862462, 1.0256994862749906, 1.0256965089548364, 1.025693573988908, 1.025690680749945, 1.0256878286201554, 1.0256850169910723, 1.0256822452634125, 1.0256795128469374, 1.0256768191603165, 1.0256741636309905, 1.0256715456950403, 1.0256689647970547, 1.0256664203900014, 1.0256639119351005, 1.025661438901699, 1.0256590007671473, 1.0256565970166776, 1.025654227143285, 1.025651890647608, 1.025649587037816, 1.0256473158294896, 1.0256450765455138, 1.0256428687159624, 1.0256406918779921, 1.0256385455757326, 1.0256364293601827, 1.0256343427891046, 1.025632285426921, 1.0256302568446158, 1.0256282566196315, 1.0256262843357742, 1.0256243395831146, 1.0256224219578933, 1.0256205310624287, 1.025618666505022, 1.0256168278998685, 1.0256150148669665, 1.02561322703203, 1.0256114640264018, 1.0256097254869676, 1.0256080110560726, 1.0256063203814363, 1.0256046531160738, 1.0256030089182133, 1.0256013874512175, 1.0255997883835053, 1.0255982113884756, 1.025596656144431, 1.025595122334503, 1.02559360964658, 1.0255921177732328, 1.0255906464116458, 1.0255891952635452, 1.02558776403513, 1.0255863524370057, 1.0255849601841152, 1.0255835869956744, 1.025582232595106, 1.025580896709977, 1.0255795790719349, 1.0255782794166453, 1.025576997483731, 1.0255757330167137, 1.0255744857629514, 1.0255732554735835, 1.0255720419034706, 1.0255708448111398, 1.025569663958728, 1.0255684991119274, 1.025567350039932, 1.0255662165153834, 1.0255650983143187, 1.0255639952161204, 1.025562907003462, 1.0255618334622634, 1.025560774381636, 1.025559729553838, 1.0255586987742251, 1.025557681841204, 1.0255566785561863, 1.025555688723541, 1.0255547121505528, 1.025553748647375, 1.0255527980269878, 1.0255518601051539, 1.0255509347003775, 1.0255500216338629, 1.0255491207294711, 1.0255482318136842, 1.0255473547155596, 1.0255464892666963, 1.025545635301193, 1.025544792655612, 1.0255439611689408, 1.0255431406825564, 1.0255423310401872, 1.0255415320878793, 1.0255407436739614, 1.0255399656490085, 1.0255391978658084, 1.0255384401793302, 1.025537692446688, 1.025536954527111, 1.0255362262819105, 1.0255355075744483, 1.0255347982701066, 1.025534098236255, 1.025533407342223, 1.0255327254592697, 1.0255320524605533, 1.0255313882211043, 1.0255307326177963, 1.0255300855293166, 1.0255294468361418, 1.0255288164205092, 1.0255281941663892, 1.0255275799594608, 1.0255269736870847, 1.0255263752382784, 1.0255257845036911, 1.0255252013755791, 1.025524625747781, 1.0255240575156952, 1.0255234965762539, 1.0255229428279027, 1.0255223961705757, 1.0255218565056752, 1.0255213237360468, 1.025520797765959, 1.0255202785010833, 1.0255197658484696, 1.0255192597165295, 1.025518760015012, 1.0255182666549854, 1.0255177795488173, 1.025517298610155, 1.0255168237539054, 1.025516354896217, 1.0255158919544614, 1.0255154348472124, 1.0255149834942328, 1.0255145378164514, 1.0255140977359483, 1.0255136631759374, 1.0255132340607482, 1.0255128103158107, 1.025512391867637, 1.0255119786438063, 1.0255115705729487, 1.02551116758473, 1.0255107696098333, 1.0255103765799487, 1.0255099884277532, 1.0255096050868997, 1.0255092264919987, 1.0255088525786091, 1.0255084832832173, 1.0255081185432298, 1.0255077582969556, 1.0255074024835928, 1.0255070510432174, 1.0255067039167682, 1.0255063610460347, 1.0255060223736439, 1.0255056878430495, 1.0255053573985162, 1.0255050309851113, 1.0255047085486897, 1.0255043900358842, 1.0255040753940936, 1.0255037645714693, 1.0255034575169069, 1.0255031541800328, 1.0255028545111955, 1.0255025584614517, 1.0255022659825601, 1.0255019770269662, 1.0255016915477964, 1.0255014094988437, 1.0255011308345614, 1.025500855510052, 1.0255005834810564, 1.0255003147039459, 1.0255000491357118, 1.0254997867339577, 1.0254995274568883, 1.0254992712633024, 1.0254990181125827, 1.0254987679646876, 1.0254985207801437, 1.0254982765200356, 1.0254980351459986, 1.0254977966202101, 1.0254975609053831, 1.0254973279647555, 1.0254970977620852, 1.0254968702616405, 1.0254966454281929, 1.0254964232270105, 1.0254962036238502, 1.0254959865849496, 1.0254957720770213, 1.0254955600672446, 1.025495350523261, 1.0254951434131632, 1.0254949387054928, 1.025494736369231, 1.025494536373794, 1.0254943386890238, 1.025494143285185, 1.0254939501329563, 1.025493759203427, 1.0254935704680874, 1.025493383898826, 1.0254931994679224, 1.0254930171480416, 1.0254928369122285, 1.0254926587339013, 1.0254924825868488, 1.0254923084452217, 1.0254921362835292, 1.0254919660766335, 1.025491797799744, 1.0254916314284133, 1.0254914669385313, 1.0254913043063203, 1.0254911435083307, 1.025490984521435, 1.0254908273228258, 1.0254906718900076, 1.025490518200795, 1.0254903662333061, 1.0254902159659607, 1.0254900673774734, 1.0254899204468502, 1.0254897751533845, 1.0254896314766528, 1.0254894893965112, 1.025489348893089, 1.0254892099467887, 1.0254890725382777, 1.025488936648488, 1.02548880225861, 1.0254886693500902, 1.0254885379046264, 1.025488407904166, 1.0254882793308993, 1.0254881521672585, 1.0254880263959136, 1.0254879019997685, 1.0254877789619579, 1.0254876572658442, 1.0254875368950143, 1.0254874178332753, 1.0254873000646527, 1.0254871835733868, 1.0254870683439292, 1.0254869543609393, 1.0254868416092842, 1.0254867300740313, 1.025486619740449, 1.0254865105940025, 1.0254864026203505, 1.0254862958053443, 1.0254861901350214, 1.0254860855956072, 1.0254859821735094, 1.0254858798553164, 1.0254857786277949, 1.025485678477886, 1.0254855793927056, 1.0254854813595382, 1.0254853843658376, 1.0254852883992227, 1.025485193447476, 1.0254850994985412, 1.02548500654052, 1.0254849145616713, 1.025484823550408, 1.0254847334952948, 1.0254846443850472, 1.0254845562085269, 1.025484468954743, 1.025484382612848, 1.0254842971721345, 1.0254842126220365, 1.0254841289521248, 1.0254840461521064, 1.0254839642118214, 1.0254838831212427, 1.0254838028704725, 1.0254837234497416, 1.0254836448494073, 1.025483567059952, 1.0254834900719803, 1.0254834138762179, 1.0254833384635111, 1.025483263824823, 1.0254831899512329, 1.0254831168339362, 1.025483044464239, 1.0254829728335608, 1.0254829019334295, 1.0254828317554823, 1.0254827622914624, 1.0254826935332195, 1.0254826254727056, 1.0254825581019769, 1.0254824914131893, 1.0254824253985986, 1.0254823600505596, 1.0254822953615226, 1.0254822313240348]\n",
      "Final weights of Perceptron are: \n",
      "\n",
      "W0:  0.526315789473727 \n",
      "\n",
      "W1:  -0.052631578947401975 \n",
      "\n",
      "W2:  -0.02631578947371866 \n",
      "\n",
      "The number of epochs are:  1000\n",
      "The error counts of the ReLU step function perceptron are:  [283.0908203125, 157.2505938631075, 92.09627513256125, 57.964621560759056, 39.74327123816157, 29.71843381461577, 23.672545869494186, 19.464899434963236, 16.507087449977497, 14.400432222536542, 12.874683642359136, 11.746541857883011, 10.89159043527633, 10.225301682684597, 9.690179687470652, 9.247056123613802, 8.86919610242531, 8.538305722527669, 8.241826791468654, 7.971102935474115, 7.720135754562132, 7.4847406275461426, 7.261973299983272, 7.049740017245536, 6.846532133131224, 6.651245186648004, 6.4630553413790235, 6.281334815629387, 6.105593845111065, 5.93544072499232, 5.770554191513858, 5.610664242366187, 5.455538741993385, 5.30497400401443, 5.158788117354921, 5.016816172998806, 4.878906813770125, 4.7449197103778396, 4.614723690293346, 4.488195330308015, 4.365217881330612, 4.245680433607265, 4.129477257833381, 4.016507276486222, 3.906673632797093, 3.799883333913183, 3.6960469512050196, 3.595078365199166, 3.496894545834026, 3.4014153610452307, 3.3085634083583013, 3.218263865388111, 3.130444356047435, 3.0450348299412853, 2.961967452933238, 2.881176507259381, 2.802598299866509, 2.7261710778864128, 2.651834950343905, 2.579531815344463, 2.509205292106714, 2.440800657301966, 2.3742647852423757, 2.309546091524811, 2.246594479791873, 2.1853612913169314, 2.1257992571582074, 2.0678624526591802, 2.0115062540998974, 1.9566872973271245, 1.9034537817151975, 1.8532018369105023, 1.8065227927112235, 1.762928588587038, 1.7221115096789554, 1.6838525334366745, 1.6479771480280332, 1.6143344190940694, 1.5827872889329353, 1.5532081980114365, 1.5254771566163812, 1.4994808949283929, 1.475112453287276, 1.452270927677892, 1.4308612518869246, 1.4107939735420385, 1.391985013819009, 1.3743554131733131, 1.3578310691688227, 1.3423424725762656, 1.3278244467721514, 1.3142158940969906, 1.3014595516128287, 1.2895017577403574, 1.2782922305491946, 1.267783857977954, 1.2579324999242545, 1.2486968019255988, 1.2400380200157946, 1.2319198562627018, 1.2243083044530858, 1.217171505376113, 1.21047961115961, 1.2042046581263133, 1.1983204476568692, 1.1928024345694745, 1.1876276225509397, 1.182774466199392, 1.178222779264128, 1.1739536486926672, 1.169949354118732, 1.1661932924473903, 1.1626699072149649, 1.1593646224214889, 1.1562637805524782, 1.1533545845246593, 1.1506250433070422, 1.1480639209844794, 1.1456606890455783, 1.1434054816906722, 1.1412890539684863, 1.1393027425622861, 1.137438429057643, 1.1356885055345936, 1.1340458423369566, 1.1325037578808768, 1.131055990373434, 1.129696671320332, 1.1284203007093485, 1.1272217237634237, 1.126096109163968, 1.1250389286513003, 1.124045937914999, 1.123113158692496, 1.1222368619994083, 1.121413552419954, 1.1206399533903384, 1.1199129934122487, 1.11922979313758, 1.1185876532692454, 1.1179840432264092, 1.117416590525777, 1.1168830708336053, 1.116381398646004, 1.1159096185577617, 1.1154658970824627, 1.115048514989011, 1.1146558601218925, 1.1142864206745768, 1.1139387788873867, 1.1136116051429976, 1.113303652434412, 1.113013751181856, 1.112740804376533, 1.1124837830305685, 1.112241721913789, 1.1120137155592003, 1.111798914520178, 1.1115965218634707, 1.1114057898830996, 1.1112260170212078, 1.1110565449827756, 1.1108967560319591, 1.1107460704585745, 1.1106039442039877, 1.110469866636334, 1.1103433584656481, 1.110223969790062, 1.1101112782648028, 1.1100048873862374, 1.1099044248837031, 1.1098095412123237, 1.1097199081404399, 1.1096352174256845, 1.1095551795741165, 1.1094795226771668, 1.109407991321501, 1.1093403455671966, 1.1092763599899294, 1.109215822783138, 1.1091585349163857, 1.1091043093463846, 1.1090529702773577, 1.1090043524676427, 1.1089583005796182, 1.1089146685702331, 1.1088733191195783, 1.1088341230951138, 1.1087969590493074, 1.1087617127485816, 1.1087282767316045, 1.108696549895082, 1.10866643710532, 1.1086378488339412, 1.1086107008162416, 1.1085849137307624, 1.1085604128987505, 1.1085371280022567, 1.108514992819705, 1.1084939449778433, 1.1084739257190375, 1.1084548796829594, 1.1084367547017653, 1.1084195016079148, 1.1084030740538502, 1.1083874283427844, 1.1083725232699089, 1.108358319973375, 1.1083447817944272, 1.108331874146129, 1.1083195643901396, 1.1083078217210391, 1.1082966170577375, 1.1082859229415196, 1.1082757134403183, 1.1082659640588264, 1.1082566516540855, 1.1082477543562113, 1.1082392514939352, 1.1082311235246676, 1.1082233519687972, 1.10821591934797, 1.108208809127096, 1.1082020056598567, 1.1081954941374985, 1.1081892605407029, 1.1081832915943508, 1.1081775747250004, 1.108172098020907, 1.1081668501944366, 1.108161820546719, 1.1081569989344082, 1.108152375738416, 1.1081479418345017, 1.1081436885656037, 1.1081396077158054, 1.1081356914858365, 1.1081319324700136, 1.108128323634539, 1.108124858297065, 1.1081215301074592, 1.1081183330296842, 1.1081152613247367, 1.1081123095345733, 1.1081094724669698, 1.108106745181248, 1.1081041229748334, 1.1081016013705758, 1.1080991761048016, 1.108096843116045, 1.1080945985344235, 1.108092438671613, 1.1080903600113914, 1.1080883592007185, 1.1080864330413116, 1.108084578481698, 1.1080827926097099, 1.1080810726453953, 1.1080794159343272, 1.108077819941275, 1.1080762822442312, 1.1080748005287633, 1.1080733725826737, 1.1080719962909513, 1.1080706696309988, 1.1080693906681132, 1.108068157551215, 1.1080669685088032, 1.1080658218451263, 1.108064715936558, 1.1080636492281637, 1.10806262023045, 1.1080616275162807, 1.108060669717959, 1.1080597455244576, 1.1080588536787936, 1.1080579929755388, 1.1080571622584574, 1.108056360418264, 1.1080555863904968, 1.1080548391534957, 1.1080541177264862, 1.1080534211677533, 1.1080527485729135, 1.1080520990732656, 1.108051471834229, 1.1080508660538508, 1.1080502809613966, 1.1080497158159983, 1.1080491699053752, 1.1080486425446125, 1.1080481330750005, 1.1080476408629267, 1.108047165298822, 1.1080467057961558, 1.1080462617904783, 1.1080458327385063, 1.108045418117253, 1.1080450174231968, 1.1080446301714875, 1.1080442558951906, 1.1080438941445634, 1.1080435444863652, 1.1080432065031987, 1.1080428797928785, 1.1080425639678304, 1.1080422586545144, 1.1080419634928769, 1.1080416781358204, 1.108041402248703, 1.1080411355088553, 1.1080408776051194, 1.1080406282374062, 1.108040387116274, 1.1080401539625222, 1.1080399285068039, 1.1080397104892539, 1.1080394996591312, 1.1080392957744796, 1.1080390986017967, 1.1080389079157227, 1.1080387234987363, 1.108038545140868, 1.1080383726394203, 1.1080382057987008, 1.10803804442977, 1.108037888350189, 1.108037737383791, 1.1080375913604485, 1.1080374501158594, 1.1080373134913346, 1.1080371813335983, 1.108037053494596, 1.1080369298313049, 1.1080368102055573, 1.1080366944838689, 1.108036582537272, 1.108036474241156, 1.1080363694751163, 1.1080362681228042, 1.108036170071786, 1.1080360752134073, 1.1080359834426587, 1.1080358946580526, 1.1080358087614974, 1.1080357256581832, 1.1080356452564657, 1.1080355674677596, 1.1080354922064308, 1.1080354193896975, 1.108035348937531, 1.1080352807725609, 1.108035214819986, 1.1080351510074868, 1.1080350892651385, 1.1080350295253327, 1.1080349717226965, 1.1080349157940184, 1.1080348616781734, 1.108034809316055, 1.1080347586505046, 1.1080347096262484, 1.1080346621898314, 1.1080346162895571, 1.1080345718754314, 1.1080345288991003, 1.1080344873137995, 1.1080344470742998, 1.108034408136854, 1.1080343704591507, 1.108034334000264, 1.108034298720609, 1.108034264581896, 1.108034231547088, 1.1080341995803584, 1.108034168647053, 1.1080341387136485, 1.1080341097477169, 1.1080340817178882, 1.108034054593816, 1.1080340283461445, 1.1080340029464735, 1.1080339783673288, 1.1080339545821318, 1.108033931565169, 1.1080339092915636, 1.108033887737249, 1.108033866878941, 1.1080338466941129, 1.1080338271609707, 1.1080338082584282, 1.1080337899660848, 1.1080337722642017, 1.1080337551336832, 1.1080337385560508, 1.1080337225134276, 1.1080337069885169, 1.1080336919645826, 1.1080336774254311, 1.1080336633553944, 1.1080336497393133, 1.108033636562518, 1.1080336238108162, 1.1080336114704743, 1.1080335995282038, 1.1080335879711467, 1.1080335767868617, 1.10803356596331, 1.1080335554888419, 1.1080335453521866, 1.1080335355424364, 1.108033526049037, 1.1080335168617763, 1.1080335079707715, 1.10803349936646, 1.1080334910395886, 1.1080334829812029, 1.1080334751826384, 1.1080334676355104, 1.1080334603317055, 1.1080334532633724, 1.1080334464229133, 1.1080334398029772, 1.1080334333964488, 1.1080334271964436, 1.1080334211962997, 1.10803341538957, 1.1080334097700155, 1.1080334043315994, 1.108033399068479, 1.1080333939750011, 1.1080333890456948, 1.1080333842752648, 1.1080333796585884, 1.1080333751907072, 1.1080333708668235, 1.1080333666822935, 1.1080333626326242, 1.1080333587134672, 1.108033354920615, 1.1080333512499942, 1.1080333476976656, 1.1080333442598145, 1.1080333409327505, 1.1080333377129024, 1.1080333345968136, 1.108033331581139, 1.1080333286626427, 1.1080333258381914, 1.1080333231047539, 1.108033320459396, 1.1080333178992794, 1.1080333154216562, 1.1080333130238675, 1.1080333107033407, 1.1080333084575853, 1.1080333062841912, 1.1080333041808261, 1.1080333021452335, 1.1080333001752292, 1.1080332982686993, 1.1080332964235984, 1.1080332946379463, 1.1080332929098275, 1.1080332912373883, 1.1080332896188336, 1.108033288052428, 1.10803328653649, 1.1080332850693937, 1.1080332836495652, 1.108033282275482, 1.108033280945669, 1.1080332796587, 1.1080332784131945, 1.108033277207817, 1.1080332760412737, 1.1080332749123136, 1.1080332738197256, 1.108033272762338, 1.1080332717390164, 1.108033270748663, 1.1080332697902162, 1.1080332688626473, 1.1080332679649616, 1.108033267096196, 1.1080332662554195, 1.1080332654417298, 1.108033264654254, 1.1080332638921473, 1.108033263154593, 1.1080332624408, 1.1080332617500024, 1.1080332610814596, 1.1080332604344543, 1.1080332598082925, 1.108033259202303, 1.1080332586158361, 1.1080332580482624, 1.1080332574989733, 1.1080332569673796, 1.1080332564529116, 1.108033255955017, 1.1080332554731624, 1.1080332550068308, 1.1080332545555223, 1.1080332541187523, 1.108033253696053, 1.108033253286971, 1.1080332528910675, 1.108033252507918, 1.1080332521371115, 1.1080332517782505, 1.10803325143095, 1.1080332510948379, 1.1080332507695532, 1.1080332504547474, 1.1080332501500834, 1.1080332498552339, 1.1080332495698824, 1.1080332492937233, 1.1080332490264608, 1.1080332487678082, 1.1080332485174877, 1.1080332482752309, 1.1080332480407782, 1.1080332478138786, 1.1080332475942882, 1.1080332473817718, 1.1080332471761014, 1.1080332469770564, 1.1080332467844236, 1.1080332465979963, 1.1080332464175746, 1.1080332462429647, 1.1080332460739801, 1.108033245910439, 1.1080332457521662, 1.108033245598992, 1.1080332454507524, 1.1080332453072876, 1.108033245168445, 1.1080332450340746, 1.1080332449040333, 1.1080332447781807, 1.1080332446563825, 1.1080332445385077, 1.1080332444244303, 1.1080332443140277, 1.1080332442071816, 1.1080332441037775, 1.1080332440037042, 1.1080332439068548, 1.108033243813125, 1.108033243722415, 1.108033243634627, 1.1080332435496671, 1.1080332434674438, 1.1080332433878695, 1.1080332433108584, 1.108033243236328, 1.1080332431641988, 1.108033243094393, 1.1080332430268358, 1.1080332429614552, 1.1080332428981803, 1.108033242836944, 1.1080332427776804, 1.1080332427203254, 1.1080332426648187, 1.1080332426110995, 1.108033242559111, 1.1080332425087975, 1.1080332424601045, 1.10803324241298, 1.1080332423673738, 1.1080332423232366, 1.1080332422805217, 1.1080332422391819, 1.1080332421991745, 1.1080332421604553, 1.1080332421229842, 1.1080332420867194, 1.108033242051623, 1.1080332420176575, 1.1080332419847858, 1.1080332419529733, 1.1080332419221854, 1.1080332418923893, 1.1080332418635532, 1.1080332418356458, 1.1080332418086376, 1.1080332417824992, 1.108033241757203, 1.1080332417327217, 1.1080332417090288, 1.1080332416860992, 1.1080332416639083, 1.1080332416424326, 1.1080332416216483, 1.1080332416015333, 1.1080332415820666, 1.108033241563227, 1.1080332415449945, 1.108033241527349, 1.108033241510272, 1.1080332414937453, 1.1080332414777507, 1.1080332414622711, 1.1080332414472909, 1.1080332414327927, 1.1080332414187617, 1.1080332414051828, 1.1080332413920413, 1.108033241379323, 1.1080332413670146, 1.1080332413551024, 1.1080332413435738, 1.1080332413324172, 1.1080332413216194, 1.10803324131117, 1.108033241301057, 1.1080332412912692, 1.1080332412817975, 1.1080332412726304, 1.1080332412637586, 1.1080332412551732, 1.108033241246864, 1.108033241238822, 1.1080332412310399, 1.1080332412235079, 1.1080332412162188, 1.1080332412091642, 1.108033241202337, 1.1080332411957299, 1.1080332411893357, 1.108033241183147, 1.108033241177158, 1.108033241171362, 1.1080332411657525, 1.1080332411603238, 1.10803324115507, 1.1080332411499856, 1.1080332411450646, 1.1080332411403024, 1.1080332411356935, 1.108033241131233, 1.1080332411269165, 1.108033241122739, 1.1080332411186955, 1.1080332411147826, 1.1080332411109963, 1.1080332411073313, 1.1080332411037845, 1.108033241100352, 1.1080332410970302, 1.1080332410938152, 1.108033241090704, 1.1080332410876927, 1.1080332410847786, 1.1080332410819584, 1.108033241079229, 1.1080332410765874, 1.108033241074031, 1.1080332410715572, 1.1080332410691627, 1.1080332410668456, 1.108033241064603, 1.1080332410624327, 1.1080332410603324, 1.1080332410582994, 1.1080332410563325, 1.1080332410544282, 1.108033241052586, 1.1080332410508027, 1.1080332410490767, 1.1080332410474067, 1.1080332410457905, 1.1080332410442262, 1.1080332410427123, 1.108033241041247, 1.1080332410398293, 1.1080332410384568, 1.108033241037129, 1.1080332410358436, 1.1080332410345997, 1.1080332410333955, 1.1080332410322307, 1.1080332410311033, 1.108033241030012, 1.1080332410289562, 1.1080332410279339, 1.108033241026945, 1.1080332410259879, 1.1080332410250615, 1.108033241024165, 1.1080332410232971, 1.1080332410224574, 1.1080332410216447, 1.1080332410208582, 1.108033241020097, 1.1080332410193603, 1.1080332410186475, 1.1080332410179579, 1.10803324101729, 1.1080332410166436, 1.1080332410160183, 1.108033241015413, 1.1080332410148275, 1.1080332410142604, 1.1080332410137121, 1.108033241013181, 1.1080332410126672, 1.10803324101217, 1.1080332410116887, 1.1080332410112226, 1.108033241010772, 1.1080332410103357, 1.1080332410099138, 1.108033241009505, 1.1080332410091094, 1.108033241008727, 1.1080332410083567, 1.108033241007998, 1.1080332410076512, 1.1080332410073157, 1.1080332410069906, 1.1080332410066762, 1.108033241006372, 1.1080332410060776, 1.1080332410057927, 1.1080332410055167, 1.1080332410052498, 1.1080332410049913, 1.1080332410047413, 1.1080332410044993, 1.108033241004265, 1.1080332410040383, 1.108033241003819, 1.1080332410036067, 1.1080332410034015, 1.1080332410032026, 1.1080332410030103, 1.1080332410028242, 1.1080332410026439, 1.1080332410024694, 1.1080332410023004, 1.1080332410021374, 1.1080332410019793, 1.108033241001826, 1.1080332410016782, 1.108033241001535, 1.108033241001396, 1.1080332410012619, 1.1080332410011318, 1.1080332410010063, 1.1080332410008848, 1.108033241000767, 1.108033241000653, 1.108033241000543, 1.1080332410004363, 1.1080332410003328, 1.1080332410002327, 1.1080332410001363, 1.1080332410000424, 1.1080332409999518, 1.1080332409998643, 1.1080332409997793, 1.1080332409996971, 1.1080332409996179, 1.1080332409995408, 1.1080332409994664, 1.1080332409993943, 1.1080332409993245, 1.108033240999257, 1.1080332409991918, 1.1080332409991287, 1.1080332409990676, 1.1080332409990084, 1.108033240998951, 1.1080332409988958, 1.108033240998842, 1.10803324099879, 1.1080332409987397, 1.108033240998691, 1.108033240998644, 1.1080332409985985, 1.1080332409985543, 1.1080332409985119, 1.1080332409984708, 1.1080332409984304, 1.108033240998392, 1.1080332409983542, 1.108033240998318, 1.1080332409982832, 1.1080332409982492, 1.1080332409982163, 1.1080332409981848, 1.1080332409981537, 1.108033240998124, 1.108033240998095, 1.1080332409980673, 1.1080332409980405, 1.1080332409980143, 1.108033240997989, 1.1080332409979645, 1.1080332409979408, 1.108033240997918, 1.1080332409978957, 1.1080332409978744, 1.108033240997854, 1.1080332409978335, 1.1080332409978142, 1.108033240997795, 1.1080332409977771, 1.1080332409977593, 1.1080332409977423, 1.1080332409977258, 1.10803324099771, 1.1080332409976945, 1.1080332409976794, 1.1080332409976648, 1.1080332409976508, 1.1080332409976372, 1.1080332409976243, 1.1080332409976115, 1.1080332409975993, 1.1080332409975873, 1.108033240997576, 1.1080332409975648, 1.108033240997554, 1.1080332409975437, 1.1080332409975335, 1.1080332409975235, 1.108033240997514, 1.108033240997505, 1.1080332409974962, 1.1080332409974876, 1.1080332409974791, 1.1080332409974711, 1.1080332409974636, 1.108033240997456, 1.1080332409974487, 1.1080332409974416, 1.1080332409974347, 1.108033240997428, 1.1080332409974218, 1.1080332409974154, 1.1080332409974096, 1.1080332409974036, 1.1080332409973983, 1.1080332409973925, 1.1080332409973876, 1.1080332409973825, 1.1080332409973777, 1.1080332409973728, 1.1080332409973683, 1.1080332409973637, 1.1080332409973594, 1.1080332409973552, 1.1080332409973512, 1.1080332409973472, 1.1080332409973437, 1.10803324099734, 1.1080332409973364, 1.108033240997333, 1.1080332409973297, 1.1080332409973266, 1.1080332409973233, 1.1080332409973201, 1.1080332409973175, 1.1080332409973146, 1.108033240997312, 1.108033240997309, 1.1080332409973068, 1.1080332409973042, 1.1080332409973017, 1.1080332409972993, 1.1080332409972973, 1.1080332409972953, 1.1080332409972928, 1.1080332409972908, 1.108033240997289, 1.108033240997287, 1.108033240997285, 1.1080332409972835, 1.1080332409972815, 1.10803324099728, 1.1080332409972784, 1.1080332409972768, 1.1080332409972753, 1.108033240997274, 1.1080332409972726, 1.108033240997271, 1.10803324099727, 1.1080332409972686, 1.1080332409972673, 1.108033240997266, 1.1080332409972649, 1.1080332409972635, 1.1080332409972626, 1.1080332409972617, 1.1080332409972606, 1.1080332409972593, 1.1080332409972586, 1.1080332409972575, 1.108033240997257, 1.108033240997256, 1.108033240997255, 1.1080332409972544, 1.1080332409972535, 1.1080332409972526, 1.108033240997252, 1.1080332409972513, 1.1080332409972506, 1.1080332409972502, 1.1080332409972493, 1.1080332409972486, 1.108033240997248, 1.1080332409972478, 1.1080332409972469, 1.1080332409972462, 1.1080332409972458, 1.1080332409972453, 1.1080332409972447, 1.1080332409972444, 1.108033240997244, 1.1080332409972435, 1.108033240997243, 1.1080332409972427, 1.108033240997242, 1.1080332409972418, 1.1080332409972413, 1.108033240997241, 1.1080332409972407, 1.1080332409972402, 1.10803324099724, 1.1080332409972398, 1.1080332409972393, 1.108033240997239, 1.1080332409972387, 1.1080332409972384, 1.1080332409972384, 1.108033240997238, 1.1080332409972375, 1.1080332409972373, 1.1080332409972373, 1.108033240997237, 1.1080332409972367, 1.1080332409972367, 1.1080332409972362, 1.1080332409972362, 1.108033240997236, 1.1080332409972358, 1.1080332409972355, 1.1080332409972355, 1.1080332409972353, 1.1080332409972349, 1.1080332409972347, 1.1080332409972349, 1.1080332409972344, 1.1080332409972344, 1.1080332409972344, 1.1080332409972338, 1.108033240997234, 1.1080332409972338, 1.1080332409972335, 1.1080332409972335, 1.1080332409972333, 1.1080332409972333, 1.1080332409972335, 1.1080332409972333, 1.108033240997233, 1.1080332409972329, 1.1080332409972327, 1.1080332409972327, 1.1080332409972327, 1.1080332409972327, 1.1080332409972327, 1.1080332409972324, 1.1080332409972324, 1.1080332409972324, 1.108033240997232, 1.108033240997232, 1.108033240997232, 1.1080332409972318, 1.108033240997232, 1.1080332409972318, 1.1080332409972318, 1.1080332409972318, 1.1080332409972318, 1.1080332409972318, 1.1080332409972315, 1.1080332409972313, 1.1080332409972313, 1.1080332409972313, 1.1080332409972313, 1.1080332409972313, 1.1080332409972313, 1.1080332409972313, 1.108033240997231]\n"
     ]
    }
   ],
   "source": [
    "# Question A2 for XOR logic\n",
    "\n",
    "# Finding the error counts of the bipolar activation function perceptron\n",
    "epochs_count_bipolar_xor, error_count_bipolar_xor = perceptron(data_gates_input, target_output_xor, weight0, weight1, weight2, learning_rate, 'bipolar')\n",
    "print('The number of epochs are: ', epochs_count_bipolar_xor)\n",
    "print('The error counts of the bipolar step function perceptron are: ', error_count_bipolar_xor)\n",
    "\n",
    "# Finding the error counts of the sigmoid activation function perceptron\n",
    "epochs_count_sigmoid_xor, error_count_sigmoid_xor = perceptron(data_gates_input, target_output_xor, weight0, weight1, weight2, learning_rate, 'sigmoid')\n",
    "print('The number of epochs are: ', epochs_count_sigmoid_xor)\n",
    "print('The error counts of the sigmoid step function perceptron are: ', error_count_sigmoid_xor)\n",
    "\n",
    "# Finding the error counts of the ReLU activation function perceptron\n",
    "epochs_count_relu_xor, error_count_relu_xor = perceptron(data_gates_input, target_output_xor, weight0, weight1, weight2, learning_rate, 'relu')\n",
    "print('The number of epochs are: ', epochs_count_relu_xor)\n",
    "print('The error counts of the ReLU step function perceptron are: ', error_count_relu_xor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final weights of Perceptron are: \n",
      "\n",
      "W0:  -0.2999999999999812 \n",
      "\n",
      "W1:  0.19999999999999998 \n",
      "\n",
      "W2:  0.15000000000000005 \n",
      "\n",
      "Final weights of Perceptron are: \n",
      "\n",
      "W0:  -0.599999999999998 \n",
      "\n",
      "W1:  0.39999999999999997 \n",
      "\n",
      "W2:  0.24999999999999978 \n",
      "\n",
      "Final weights of Perceptron are: \n",
      "\n",
      "W0:  -0.2000000000000009 \n",
      "\n",
      "W1:  0.2 \n",
      "\n",
      "W2:  0.1500000000000002 \n",
      "\n",
      "Final weights of Perceptron are: \n",
      "\n",
      "W0:  -1.2000000000000044 \n",
      "\n",
      "W1:  1.0000000000000002 \n",
      "\n",
      "W2:  0.4500000000000002 \n",
      "\n",
      "Final weights of Perceptron are: \n",
      "\n",
      "W0:  -1.5 \n",
      "\n",
      "W1:  1.2 \n",
      "\n",
      "W2:  0.75 \n",
      "\n",
      "Final weights of Perceptron are: \n",
      "\n",
      "W0:  -1.999999999999997 \n",
      "\n",
      "W1:  1.4 \n",
      "\n",
      "W2:  1.0499999999999998 \n",
      "\n",
      "Final weights of Perceptron are: \n",
      "\n",
      "W0:  -1.9 \n",
      "\n",
      "W1:  1.5999999999999999 \n",
      "\n",
      "W2:  0.6500000000000004 \n",
      "\n",
      "Final weights of Perceptron are: \n",
      "\n",
      "W0:  -2.0000000000000004 \n",
      "\n",
      "W1:  1.8 \n",
      "\n",
      "W2:  0.8499999999999996 \n",
      "\n",
      "Final weights of Perceptron are: \n",
      "\n",
      "W0:  -2.600000000000002 \n",
      "\n",
      "W1:  2.0 \n",
      "\n",
      "W2:  1.0500000000000003 \n",
      "\n",
      "Final weights of Perceptron are: \n",
      "\n",
      "W0:  -3 \n",
      "\n",
      "W1:  2.2 \n",
      "\n",
      "W2:  1.25 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Question A3 for XOR logic\n",
    "\n",
    "learning_rates = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "epochs_count_learning_xor =[]\n",
    "for i in learning_rates:\n",
    "    epochs_count_step, error_count_step = perceptron(data_gates_input, target_output_and, weight0, weight1, weight2, i, 'step')\n",
    "    epochs_count_learning_xor.append(epochs_count_step)\n",
    "\n",
    "# Plotting epochs counts against learning rates\n",
    "plt.plot(epochs_count_learning_xor, learning_rates, color = 'orange')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Learning rates')\n",
    "plt.title('Epoch count vs Learning rate for XOR logic')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Customer</th>\n",
       "      <th>Candies (#)</th>\n",
       "      <th>Mangoes (kg)</th>\n",
       "      <th>Milk Packets (#)</th>\n",
       "      <th>Payment (Rs)</th>\n",
       "      <th>High Value Tx?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C_1</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>386</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C_2</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>289</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C_3</td>\n",
       "      <td>27</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>393</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C_4</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>110</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C_5</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>280</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>C_6</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>167</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>C_7</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>271</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>C_8</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>274</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>C_9</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>148</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>C_10</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>198</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Customer  Candies (#)  Mangoes (kg)  Milk Packets (#)  Payment (Rs)  \\\n",
       "0      C_1           20             6                 2           386   \n",
       "1      C_2           16             3                 6           289   \n",
       "2      C_3           27             6                 2           393   \n",
       "3      C_4           19             1                 2           110   \n",
       "4      C_5           24             4                 2           280   \n",
       "5      C_6           22             1                 5           167   \n",
       "6      C_7           15             4                 2           271   \n",
       "7      C_8           18             4                 2           274   \n",
       "8      C_9           21             1                 4           148   \n",
       "9     C_10           16             2                 4           198   \n",
       "\n",
       "  High Value Tx?  \n",
       "0            Yes  \n",
       "1            Yes  \n",
       "2            Yes  \n",
       "3             No  \n",
       "4            Yes  \n",
       "5             No  \n",
       "6            Yes  \n",
       "7            Yes  \n",
       "8             No  \n",
       "9             No  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question A5:\n",
    "# Applying the perceptron with a sigmoid function with the weights and learning rate of our choice\n",
    "\n",
    "# The dataframe of the customer data\n",
    "dataframe_customer_data = {\n",
    "    'Customer': ['C_1', 'C_2', 'C_3', 'C_4', 'C_5', 'C_6', 'C_7', 'C_8', 'C_9', 'C_10'],\n",
    "    'Candies (#)': [20, 16, 27, 19, 24, 22, 15, 18, 21, 16],\n",
    "    'Mangoes (kg)': [6, 3, 6, 1, 4, 1, 4, 4, 1, 2],\n",
    "    'Milk Packets (#)': [2, 6, 2, 2, 2, 5, 2, 2, 4, 4],\n",
    "    'Payment (Rs)': [386, 289, 393, 110, 280, 167, 271, 274, 148, 198],\n",
    "    'High Value Tx?': ['Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'No', 'No']\n",
    "}\n",
    "\n",
    "dataframe_customer_data = pd.DataFrame(dataframe_customer_data)\n",
    "dataframe_customer_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Customer</th>\n",
       "      <th>Candies (#)</th>\n",
       "      <th>Mangoes (kg)</th>\n",
       "      <th>Milk Packets (#)</th>\n",
       "      <th>Payment (Rs)</th>\n",
       "      <th>High Value Tx?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C_1</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>386</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C_2</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>289</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C_3</td>\n",
       "      <td>27</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>393</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C_4</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>110</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C_5</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>280</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>C_6</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>167</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>C_7</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>271</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>C_8</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>274</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>C_9</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>148</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>C_10</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>198</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Customer  Candies (#)  Mangoes (kg)  Milk Packets (#)  Payment (Rs)  \\\n",
       "0      C_1           20             6                 2           386   \n",
       "1      C_2           16             3                 6           289   \n",
       "2      C_3           27             6                 2           393   \n",
       "3      C_4           19             1                 2           110   \n",
       "4      C_5           24             4                 2           280   \n",
       "5      C_6           22             1                 5           167   \n",
       "6      C_7           15             4                 2           271   \n",
       "7      C_8           18             4                 2           274   \n",
       "8      C_9           21             1                 4           148   \n",
       "9     C_10           16             2                 4           198   \n",
       "\n",
       "   High Value Tx?  \n",
       "0               1  \n",
       "1               1  \n",
       "2               1  \n",
       "3               0  \n",
       "4               1  \n",
       "5               0  \n",
       "6               1  \n",
       "7               1  \n",
       "8               0  \n",
       "9               0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying the label encoding on the output of the dataframe\n",
    "from sklearn import preprocessing as prep\n",
    "\n",
    "label_encoder = prep.LabelEncoder()\n",
    "\n",
    "columns = ['High Value Tx?']\n",
    "for cols in columns:\n",
    "    dataframe_customer_data[cols]= label_encoder.fit_transform(dataframe_customer_data[cols])\n",
    "dataframe_customer_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the function to apply a percetron on the dataframe\n",
    "def perceptron_dataframe(dataframe_input, dataframe_output, w0, w1, w2, w3, alpha, function):\n",
    "    error_count = []\n",
    "    epochs_count = 0\n",
    "    converged = False\n",
    "    while not converged and epochs_count != 1000:\n",
    "        total_error = 0\n",
    "        for i in range(len(data_gates_input)):\n",
    "            input_1 = dataframe_input.iloc[i, 0]\n",
    "            input_2 = dataframe_input.iloc[i, 1]\n",
    "            input_3 = dataframe_input.iloc[i, 2]\n",
    "            input_4 = dataframe_input.iloc[i, 3]\n",
    "            perceptor_equation = w0 * input_1 + w1 * input_2 + w2 * input_3 + w3 * input_4\n",
    "            if function == 'step':\n",
    "                actual_output = activation_step_function(perceptor_equation)\n",
    "            if function == 'bipolar':\n",
    "                actual_output = activation_bipolar_function(perceptor_equation)\n",
    "            if function == 'sigmoid':\n",
    "                actual_output = activation_sigmoid_function(perceptor_equation)\n",
    "            if function == 'relu':\n",
    "                actual_output = activation_relu_function(perceptor_equation)\n",
    "            error_value = dataframe_output.iloc[i] - actual_output\n",
    "            total_error += error_value ** 2\n",
    "\n",
    "            w0 += alpha * error_value * input_1\n",
    "            w1 += alpha * error_value * input_2\n",
    "            w2 += alpha * error_value * input_3\n",
    "            w3 += alpha * error_value * input_4\n",
    "        error_count.append(total_error)\n",
    "        if total_error <= 0.002:\n",
    "            converged = True\n",
    "        epochs_count += 1\n",
    "    print('Final weights of Perceptron for the dataframe are: \\n')\n",
    "    print('W0: ', w0, '\\n')\n",
    "    print('W1: ', w1, '\\n')\n",
    "    print('W2: ', w2, '\\n')\n",
    "    print('W3: ', w3, '\\n')\n",
    "    return epochs_count, error_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final weights of Perceptron for the dataframe are: \n",
      "\n",
      "W0:  -163.39999999999992 \n",
      "\n",
      "W1:  9.499999999999996 \n",
      "\n",
      "W2:  -16.800000000000004 \n",
      "\n",
      "W3:  24.10000000000005 \n",
      "\n",
      "The number of epochs are:  32\n",
      "The error counts of the sigmoid step function perceptron are:  [2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1.0, 0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pavan\\AppData\\Local\\Temp/ipykernel_10148/3359918862.py:3: RuntimeWarning: overflow encountered in double_scalars\n",
      "  return 1 / (1 + math.e ** (-number))\n"
     ]
    }
   ],
   "source": [
    "# Applying the perceptron of the dataframe given\n",
    "\n",
    "weight0 = 0.2\n",
    "weight1 = 0.3\n",
    "weight2 = 0.8\n",
    "weight3 = -1.5\n",
    "learning_rate = 0.4\n",
    "dataframe_input_perceptron = dataframe_customer_data.iloc[:, 1:5]\n",
    "dataframe_output_perceptron = dataframe_customer_data.iloc[:, 5]\n",
    "\n",
    "# Finding the error counts of the sigmoid activation function perceptron on the dataframe\n",
    "epochs_count_sigmoid_data, error_count_sigmoid_data = perceptron_dataframe(dataframe_input_perceptron, dataframe_output_perceptron, weight0, weight1, weight2, weight3, learning_rate, 'sigmoid')\n",
    "print('The number of epochs are: ', epochs_count_sigmoid_data)\n",
    "print('The error counts of the sigmoid step function perceptron are: ', error_count_sigmoid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question A6:\n",
    "# Comparing the pseudo-inverse matrix application on the dataframe of customer data\n",
    "\n",
    "def pseudo_inverse_method(dataframe_input, dataframe_output):\n",
    "    matrix_input = np.matrix(dataframe_input)\n",
    "    matrix_output = np.matrix(dataframe_output).reshape(10, 1)\n",
    "    pseudo_inverse_matrix_input = np.matrix(np.linalg.pinv(matrix_input))\n",
    "    matrix_solution = np.matmul(pseudo_inverse_matrix_input, matrix_output)\n",
    "    return matrix_solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The solution for the dataframe using the pseudo-inverse matrix method is: \n",
      " [[-0.02423666]\n",
      " [ 0.01257911]\n",
      " [-0.03683543]\n",
      " [ 0.00457696]]\n"
     ]
    }
   ],
   "source": [
    "# Getting the solution of the weights that have to applied for each attribute\n",
    "dataframe_input_psinv = dataframe_customer_data.iloc[:, 1:5]\n",
    "dataframe_output_psinv = dataframe_customer_data.iloc[:, 5]\n",
    "\n",
    "dataframe_solution = pseudo_inverse_method(dataframe_input_psinv, dataframe_output_psinv)\n",
    "print('The solution for the dataframe using the pseudo-inverse matrix method is: \\n', dataframe_solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "Predicted Output:\n",
      "[[0.25216543]\n",
      " [0.25209223]\n",
      " [0.25196733]\n",
      " [0.25190172]]\n",
      "Converged after  1000  epochs.\n"
     ]
    }
   ],
   "source": [
    "# Question A7:\n",
    "# Creating a two layer neural network for AND gate logic\n",
    "\n",
    "# Input data for the AND gate\n",
    "data_input_and = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "\n",
    "# Target output for the AND gate\n",
    "data_output_and = np.array([[0], [0], [0], [1]])\n",
    "\n",
    "# Initialize random weights for the network\n",
    "np.random.seed(1)\n",
    "input_neurons = 2\n",
    "hidden1_neurons = 2\n",
    "hidden2_neurons = 2  # Renamed Hidden1 to Hidden2 for clarity\n",
    "output_neurons = 1\n",
    "learning_rate = 0.05\n",
    "\n",
    "# Initialize weights and biases with more descriptive names\n",
    "weights_input_hidden1 = np.random.uniform(size=(input_neurons, hidden1_neurons))\n",
    "bias_hidden1 = np.random.uniform(size=(1, hidden1_neurons))\n",
    "weights_hidden1_hidden2 = np.random.uniform(size=(hidden1_neurons, hidden2_neurons))\n",
    "bias_hidden2 = np.random.uniform(size=(1, hidden2_neurons))\n",
    "weights_hidden2_output = np.random.uniform(size=(hidden2_neurons, output_neurons))\n",
    "bias_output = np.random.uniform(size=(1, output_neurons))\n",
    "\n",
    "# Training the neural network\n",
    "epoch_counts = 1000\n",
    "for epoch in range(1, epoch_counts+1):\n",
    "    # Forward propagation\n",
    "    hidden1_input = np.dot(data_input_and, weights_input_hidden1) + bias_hidden1\n",
    "    hidden1_output = activation_sigmoid_function(hidden1_input)\n",
    "    hidden2_input = np.dot(hidden1_output, weights_hidden1_hidden2) + bias_hidden2\n",
    "    hidden2_output = activation_sigmoid_function(hidden2_input)\n",
    "    output_input = np.dot(hidden2_output, weights_hidden2_output) + bias_output\n",
    "    predicted_output = activation_sigmoid_function(output_input)\n",
    "\n",
    "    # Calculate the error\n",
    "    error = data_output_and - predicted_output\n",
    "\n",
    "    # Backpropagation\n",
    "    delta_output = error * activation_sigmoid_derivative_function(predicted_output)\n",
    "    error_hidden2 = delta_output.dot(weights_hidden2_output.T)\n",
    "    delta_hidden2 = error_hidden2 * activation_sigmoid_derivative_function(hidden2_output)\n",
    "    error_hidden1 = delta_hidden2.dot(weights_hidden1_hidden2.T)\n",
    "    delta_hidden1 = error_hidden1 * activation_sigmoid_derivative_function(hidden1_output)\n",
    "\n",
    "    # Update weights and biases\n",
    "    weights_hidden2_output += hidden2_output.T.dot(delta_output) * learning_rate\n",
    "    weights_hidden1_hidden2 += hidden1_output.T.dot(delta_hidden2) * learning_rate\n",
    "    weights_input_hidden1 += data_input_and.T.dot(delta_hidden1) * learning_rate\n",
    "    bias_output += np.sum(delta_output, axis=0) * learning_rate\n",
    "    bias_hidden2 += np.sum(delta_hidden2, axis=0) * learning_rate\n",
    "    bias_hidden1 += np.sum(delta_hidden1, axis=0) * learning_rate\n",
    "\n",
    "    # Check for convergence\n",
    "    if np.mean(np.abs(error)) <= 0.002:\n",
    "        break\n",
    "\n",
    "# Test the trained network\n",
    "test_input = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "predicted_output = activation_sigmoid_function(np.dot( activation_sigmoid_function(np.dot( activation_sigmoid_function(np.dot(test_input, weights_input_hidden1) + bias_hidden1), weights_hidden1_hidden2) + bias_hidden2), weights_hidden2_output) + bias_output)\n",
    "\n",
    "print(\"Input:\")\n",
    "print(test_input)\n",
    "print(\"Predicted Output:\")\n",
    "print(predicted_output)\n",
    "print(\"Converged after \", epoch_counts, \" epochs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "Predicted Output:\n",
      "[[0.25216543]\n",
      " [0.25209223]\n",
      " [0.25196733]\n",
      " [0.25190172]]\n",
      "Converged after  1000  epochs.\n"
     ]
    }
   ],
   "source": [
    "# Question A8:\n",
    "# Applying th same function for XOR logic\n",
    "\n",
    "# Input data for the XOR gate\n",
    "data_input_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "\n",
    "# Target output for the XOR gate\n",
    "data_output_xor = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Initialize random weights for the network\n",
    "np.random.seed(1)\n",
    "input_neurons = 2\n",
    "hidden_layer1_neurons = 2\n",
    "hidden_layer2_neurons = 2\n",
    "output_neurons = 1\n",
    "learning_rate = 0.05\n",
    "\n",
    "# Initialize weights and biases with more descriptive names\n",
    "weights_input_hidden1 = np.random.uniform(size=(input_neurons, hidden_layer1_neurons))\n",
    "bias_hidden1 = np.random.uniform(size=(1, hidden_layer1_neurons))\n",
    "weights_hidden1_hidden2 = np.random.uniform(size=(hidden_layer1_neurons, hidden_layer2_neurons))\n",
    "bias_hidden2 = np.random.uniform(size=(1, hidden_layer2_neurons))\n",
    "weights_hidden2_output = np.random.uniform(size=(hidden_layer2_neurons, output_neurons))\n",
    "bias_output = np.random.uniform(size=(1, output_neurons))\n",
    "\n",
    "# Training the neural network\n",
    "epoch_counts = 1000\n",
    "for epoch in range(1, epoch_counts+1):\n",
    "    # Forward propagation\n",
    "    hidden1_input = np.dot(data_input_and, weights_input_hidden1) + bias_hidden1\n",
    "    hidden1_output = activation_sigmoid_function(hidden1_input)\n",
    "    hidden2_input = np.dot(hidden1_output, weights_hidden1_hidden2) + bias_hidden2\n",
    "    hidden2_output = activation_sigmoid_function(hidden2_input)\n",
    "    output_input = np.dot(hidden2_output, weights_hidden2_output) + bias_output\n",
    "    predicted_output = activation_sigmoid_function(output_input)\n",
    "\n",
    "    # Calculate the error\n",
    "    error_value = data_output_and - predicted_output\n",
    "\n",
    "    # Backpropagation\n",
    "    delta_output = error_value * activation_sigmoid_derivative_function(predicted_output)\n",
    "    error_hidden2 = delta_output.dot(weights_hidden2_output.T)\n",
    "    delta_hidden2 = error_hidden2 * activation_sigmoid_derivative_function(hidden2_output)\n",
    "    error_hidden1 = delta_hidden2.dot(weights_hidden1_hidden2.T)\n",
    "    delta_hidden1 = error_hidden1 * activation_sigmoid_derivative_function(hidden1_output)\n",
    "\n",
    "    # Update weights and biases\n",
    "    weights_hidden2_output += hidden2_output.T.dot(delta_output) * learning_rate\n",
    "    weights_hidden1_hidden2 += hidden1_output.T.dot(delta_hidden2) * learning_rate\n",
    "    weights_input_hidden1 += data_input_and.T.dot(delta_hidden1) * learning_rate\n",
    "    bias_output += np.sum(delta_output, axis=0) * learning_rate\n",
    "    bias_hidden2 += np.sum(delta_hidden2, axis=0) * learning_rate\n",
    "    bias_hidden1 += np.sum(delta_hidden1, axis=0) * learning_rate\n",
    "\n",
    "    # Check for convergence\n",
    "    if np.mean(np.abs(error)) <= 0.002:\n",
    "        break\n",
    "\n",
    "# Test the trained network\n",
    "test_input = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "predicted_output = activation_sigmoid_function(np.dot( activation_sigmoid_function(np.dot( activation_sigmoid_function(np.dot(test_input, weights_input_hidden1) + bias_hidden1), weights_hidden1_hidden2) + bias_hidden2), weights_hidden2_output) + bias_output)\n",
    "\n",
    "print(\"Input:\")\n",
    "print(test_input)\n",
    "print(\"Predicted Output:\")\n",
    "print(predicted_output)\n",
    "print(\"Converged after \", epoch_counts, \" epochs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question A9:\n",
    "# Performing A1 and A2 questions on the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(activation='logistic', hidden_layer_sizes=(2,),\n",
      "              learning_rate_init=0.05, max_iter=1000, random_state=42)\n",
      "The predicted output for AND logic is:  [0 0 0 1]\n",
      "The predicted output for AND logic is:  [0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Question A10:\n",
    "# Creating the dataframe for the AND logic\n",
    "from sklearn import neural_network\n",
    "\n",
    "dataframe_and = {\n",
    "    'A': [0, 0, 1, 1],\n",
    "    'B': [0, 1, 0, 1],\n",
    "    'Output': [0, 0, 0, 1]\n",
    "}\n",
    "# Creating the dataframe for the XOR logic\n",
    "dataframe_xor = {\n",
    "    'A': [0, 0, 1, 1],\n",
    "    'B': [0, 1, 0, 1],\n",
    "    'Output': [0, 1, 1, 0]\n",
    "}\n",
    "dataframe_and = pd.DataFrame(dataframe_and)\n",
    "dataframe_xor = pd.DataFrame(dataframe_xor)\n",
    "\n",
    "perceptron_model = neural_network.MLPClassifier(hidden_layer_sizes=(2,), activation='logistic', max_iter=1000, learning_rate_init=0.05, random_state=42)\n",
    "# Applying the perceptron MLP model on AND logic\n",
    "output_and = perceptron_model.fit(dataframe_and.iloc[:, 0:2], dataframe_and.iloc[:, 2])\n",
    "print(output_and)\n",
    "output_AND_predicted = perceptron_model.predict(dataframe_and.iloc[:, 0:2])\n",
    "print(\"The predicted output for AND logic is: \", output_AND_predicted)\n",
    "\n",
    "# Applying the perceptron MLP model on XOR logic\n",
    "output_xor = perceptron_model.fit(dataframe_xor.iloc[:, 0:2], dataframe_xor.iloc[:, 2])\n",
    "output_XOR_predicted = perceptron_model.predict(dataframe_xor.iloc[:, 0:2])\n",
    "print(\"The predicted output for AND logic is: \", output_XOR_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of samples and featuresis:\n",
      "  (16384, 13) (16384, 1)\n",
      "(16384, 13) (16384,)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16384 entries, 0 to 16383\n",
      "Data columns (total 14 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       16384 non-null  float64\n",
      " 1   1       16384 non-null  float64\n",
      " 2   2       16384 non-null  float64\n",
      " 3   3       16384 non-null  float64\n",
      " 4   4       16384 non-null  float64\n",
      " 5   5       16384 non-null  float64\n",
      " 6   6       16384 non-null  float64\n",
      " 7   7       16384 non-null  float64\n",
      " 8   8       16384 non-null  float64\n",
      " 9   9       16384 non-null  float64\n",
      " 10  10      16384 non-null  float64\n",
      " 11  11      16384 non-null  float64\n",
      " 12  12      16384 non-null  float64\n",
      " 13  13      16384 non-null  uint8  \n",
      "dtypes: float64(13), uint8(1)\n",
      "memory usage: 1.6 MB\n"
     ]
    }
   ],
   "source": [
    "# Question A11:\n",
    "# Applying the project dataframe on the MLP classifier\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import preprocessing as prepro \n",
    "\n",
    "#Insert your features and labels\n",
    "samples = np.load(r'activation_maps_lines_image52_pixel.npy')\n",
    "features = np.load(r'activation_maps_lines_image52_labelpixel.npy')\n",
    "\n",
    "print(\"The shape of samples and featuresis:\\n \",samples.shape,features.shape)\n",
    "# Reshape your data  (pixels x features)\n",
    "samples_num = samples.shape[0]   # Total number of pixels\n",
    "features_num = samples.shape[1]  # Number of features for each pixel\n",
    "X_reshaped = samples.reshape(samples_num,features_num)\n",
    "y_reshaped = features.reshape(samples_num)  # Assuming y is your pixel-wise label mask\n",
    "print(X_reshaped.shape,y_reshaped.shape)\n",
    "samples = pd.DataFrame(samples,columns=np.arange(0,13))\n",
    "samples['13']=features # Adding features coulmn into the samples dataset \n",
    "samples.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the MLP classifier on the project data is:  0.5593978844589097\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1392\n",
      "           1       0.00      0.00      0.00       180\n",
      "         128       0.00      0.00      0.00       594\n",
      "         255       0.56      1.00      0.72      2750\n",
      "\n",
      "    accuracy                           0.56      4916\n",
      "   macro avg       0.14      0.25      0.18      4916\n",
      "weighted avg       0.31      0.56      0.40      4916\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Applying the neural network on the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "mlp_model = neural_network.MLPClassifier(hidden_layer_sizes=(2,), activation='logistic', max_iter=1000, learning_rate_init=0.05, random_state=42)\n",
    "\n",
    "input_data_project = samples.iloc[:, 0:13]\n",
    "output_data_project = samples.iloc[:, 13]\n",
    "input_project_data_train, input_project_data_test, output_project_data_train, output_project_data_test = train_test_split(input_data_project, output_data_project, test_size=0.3, random_state=42)\n",
    "\n",
    "mlp_model.fit(input_project_data_train, output_project_data_train)\n",
    "predicted_output = mlp_model.predict(input_project_data_test)\n",
    "accuracy_value_mlp = accuracy_score(predicted_output, output_project_data_test)\n",
    "print(\"The accuracy of the MLP classifier on the project data is: \", accuracy_value_mlp)\n",
    "print(classification_report(output_project_data_test, predicted_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
